<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110623657-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110623657-1');
  </script>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron, Deepak Pathak, Aditya Kusupati and Jeff Donahue*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder {
    border-width: 1px;
    border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Gautham Krishna Gudur</title>
  <meta name="Gautham Krishna Gudur's Homepage" http-equiv="Content-Type" content="Gautham Krishna Gudur's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <script src="js/scramble.js"></script>
</head>


<body>

<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Gautham Krishna Gudur</font><br> -->
    <pageheading>Gautham Krishna Gudur</pageheading><br>
    <b>email:</b> gauthamkrishna [at] utexas [dot] edu<br>
    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; gauthamkrishna [dot] gudur [at] gmail [dot] com
    <!--<font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'dithgaag.kiamlrmo@auursguh.cmn', 
        [18,25,4,12,16,6,24,1,27,8,10,14,7,26,20,23,29,21,2,17,19,9,11,22,3,5,15,28,30,13]);
    </script>-->
  </p>

  <tr>
    <td width="32%" valign="top"><img src="images/avatar-icon.png" width="100%" style="border-radius:15px">
    <p align="center">
    <a href="CV_Gautham.pdf" target="_blank">CV</a> /
    <a href="https://scholar.google.co.in/citations?user=X5ThCEAAAAAJ" target="_blank">Google Scholar</a> <br/>
    <a href="https://www.linkedin.com/in/gauthamkrishna-g/" target="_blank">LinkedIn</a> /
    <a href="https://twitter.com/gauthamkrishna_" target="_blank">Twitter</a> <br/>
    <a href="https://github.com/gauthamkrishna-g" target="_blank">Github</a> /
    <a href="https://www.hackerrank.com/gauthamkrishna_g" target="_blank">HackerRank</a>
    </p>
    <p align="center"></p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am a Ph.D. student at <a href="https://www.utexas.edu/" target="_blank">The University of Texas at Austin</a> in the <a href="https://www.ece.utexas.edu/" target="_blank">Department of ECE</a>, advised by the wonderful <a href="https://www.ideal.ece.utexas.edu/~ghosh/" target="_blank">Prof. Joydeep Ghosh</a> (previously by <a href="https://users.ece.utexas.edu/~ethomaz/" target="_blank"> Prof. Edison Thomaz</a>). I've also been affiliated with <a href="https://wncg.org/" target="_blank">WNCG</a>, <a href="https://www.ifml.institute/" target="_blank">IFML</a>, and <a href="https://imagine.utexas.edu/" target="_blank">iMAGiNE</a>. My research focuses on resource-efficient and data-centric machine learning, multimodal sensing, and human-centered AI.</p>
    <p>Previously, I interned at <a href="https://www.nokia.com/bell-labs/about/locations/cambridge-uk/" target="_blank">Bell Labs, Cambridge, UK</a> (<a href="https://www.nokia.com/bell-labs/research/sdsr/device-software/" target="_blank">Device Intelligence team</a>, led by <a href="https://www.fahim-kawsar.net/" target="_blank">Dr. Fahim Kawsar</a>), advancing foundation models for health sensing. Before my Ph.D., I was a Data Scientist at <a href="https://www.ericsson.com/" target="_blank">Ericsson R&D</a>, researching AI/ML solutions for next-generation telecom systems, and at <a href="https://www.smartcardia.com/" target="_blank">SmartCardia</a> – an <a href="https://www.epfl.ch/en/" target="_blank">EPFL</a> spin-off in AI-driven wearable healthcare.</p>
    <!--<font size=1>(Hit me up for interesting collaborations!)</font>-->
    <p>In a past life, I was a Research Assistant at <a href="https://www.solarillionfoundation.org/" target="_blank">Solarillion Foundation</a>, exploring on-device ML. I earned my Bachelor's from <a href="https://www.ssn.edu.in/" target="_blank">SSN College of Engineering</a>, where I also conducted early research in ML, IoT, and HCI.</p>
    <p>Outside research, I enjoy traveling, exploring Indian classical and world music, badminton, and DotA.</p>

<!--
<p align=center>
    <a href="Resume_Gautham.pdf" target="_blank">CV</a> /
    <a href="mailto:gauthamkrishna.gudur@gmail.com">Email</a> /
    <a href="bio.txt" target="_blank">Bio</a> /
    <a href="https://www.linkedin.com/in/gauthamkrishna-g/" target="_blank">LinkedIn</a> /
    <a href="https://scholar.google.co.in/citations?user=X5ThCEAAAAAJ" target="_blank">Google Scholar</a> /
    <a href="https://github.com/gauthamkrishna-g" target="_blank">Github</a> /
    <a href="https://twitter.com/gauthamkrishna_" target="_blank">Twitter</a> /
    <a href="https://www.hackerrank.com/gauthamkrishna_g" target="_blank">HackerRank</a>
</p>
-->
    </td>
  </tr>
</table>


<!--<p align="center">-->
<center>
[ <a href="#research_interests">Research Interests</a> | <a href="#news">News</a> | <a href="#publications">Publications</a> | <a href="#patents">Patents</a> | <a href="#services">Services</a> | <a href="#honorsandawards">Honors/Awards</a> | <a href="#talks">Talks</a> | <a href="#summer_schools">Summer Schools</a> | <a href="#certifications">MOOCs</a> ]
</center><br>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id='research_interests'>&nbsp;&nbsp;Research Interests</sectionheading><br>
     <!--Interdisciplinary with a good mix of theory/applied ML across multiple data regimes and domains.<br>-->
     <!--&nbsp;&nbsp;&nbsp;I'm broadly interested in the intersection of resource-efficient/human-centered learning and Generative AI, with a particular focus on:<br>-->
     <ul>
        <li> Foundation models and LLMs: Leveraging data- and parameter-efficient learning and reasoning</li>
        <li> Multimodal sensing (health, inertial, acoustic, wearable), learning from time-series for health, ubiquitous computing (human activity recognition), and human-centered AI with a focus on real-world deployability
        <li> Accelerating training and inference using data-centric approaches by prioritizing important samples, in continual (curriculum/few-shot) and data-efficient (human-in-the-loop) settings
        <li> Federated learning under statistical heterogeneities with new labels and models
        <li> Learning from limited supervision, particularly under data/label scarcity, sparsity, and calibrated uncertainty
        <!--<li> Leveraging explainable components for guided neural network training-->
      </ul>
      Broadly, <b>resource-efficient</b> and <b>resource-aware learning</b>,<br>
      where <b>resource := data, sample, label, model, parameter, compute, etc.</b><br>

      <p>Here are a <a href="javascript:toggleblock('research_keywords')">few keywords</a> that might best describe my research interests <font size=1>(present/past, hopefully in the future!)</font>.
        <div id="research_keywords" style="display:none">
          <b>
            <ul>
              <li>Machine (Continual, Active, Federated, Curriculum, Few-shot, Self-supervised) Learning</li>
              <li>Foundation Models, LLMs, Generative AI, Parameter-Efficient Fine-Tuning (PEFT), Time Series for Health </li>
              <li>Multimodal Sensing (health, inertial, acoustic, wearable), Ubiquitous Computing, Human Activity Recognition, On-Device Training</li>
              <li>Limited Supervision, Test-time Adaptation, Subset/Model Selection, Uncertainty-Aware Learning, Sparsity</li>
            </ul>
          </b>
        </div></p>
      <!--I've published my works at conferences and workshop venues like ACM UbiComp/ISWC, NeurIPS, Interspeech, ICML, ICLR, KDD, MobiSys-->
    </td></tr>
</table>


<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr>
    <td width="33%" valign="top" align="center"><a href="publications/wordcloud_research.png" target="_blank"><img src="publications/wordcloud_research.png" alt="sym" width="100%" style="border-radius:15px;"></a><br>
      This is what <a href="http://wordle.net" target="_blank">Wordle</a> thinks of my publication titles.
    </td>
    <td width="67%" valign="top">
      <p>
      Broadly, <b>resource-efficient</b> and <b>resource-aware learning</b>,<br>
      where <b>resource := data, sample, label, model, parameter, compute, etc.</b><br>
      </p>
      <p><a href="javascript:toggleblock('research_keywords')">Few Keywords</a> that might best describe my research interests<font size=1><br>(present/past, hopefully in the future!)</font>.
        <div id="research_keywords" style="display:none">
          <b>
            <ul>
              <li>Machine (Deep, Bayesian, Active, Continual, Federated, Curriculum, Few-shot, Human-in-the-Loop) Learning</li>
              <li>Generative AI, LLMs, Foundation Models</li>
              <li>Limited Supervision, Subset/Model Selection</li>
              <li>Ubiquitous Computing (Mobile/Wearable Sensing, Human Activity Recognition, Audio/Cross-modal Sensing) and On-Device Training</li>
            </ul>
          </b>
        </div></p>
    </td>
  </tr>
</table>-->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

  <tr><td>
    <sectionheading id='news'>&nbsp;&nbsp;What's New!</sectionheading>
    <ul>
      <li> I joined <a href="https://www.nokia.com/bell-labs/about/locations/cambridge-uk/" target="_blank">Bell Labs, Cambridge, UK</a> as a research intern in Summer 2025 to work on AI health sensing!</li>
      <li> <a href="#AttenGluco25">AttenGluco: Multimodal Transformer-Based Blood Glucose Forecasting</a> accepted at IEEE EMBC 2025.</li>
      <li> I was funded by the NSF ExpandAI Grant for my Ph.D. research on continual learning for pervasive systems.</li>
      <li> <a href="#ENLSP24_NeurIPS">Dataset Distillation for Audio Classification: A Data-Efficient Alternative to Active Learning</a> accepted at the Efficient Natural Language and Speech Processing (ENLSP) workshop @ NeurIPS 2024.</li>
      <li> <a href="#SVFT24">SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors</a> accepted at <b>NeurIPS 2024</b>. Abridged versions accepted at FITML workshop @ NeurIPS 2024, and WANT (<i><b>oral</b></i>) and ES-FoMo workshops @ ICML 2024.</li>
      <li> I was awarded the Graduate Ph.D. Fellowship from Cockrell School of Engineering at UT Austin.</li>
      <li> I will be joining <a href="https://www.utexas.edu/" target="_blank"> The University of Texas at Austin</a> in August 2023 for a Ph.D. in the <a href="https://ece.utexas.edu/" target="_blank">Chandra Department of ECE</a>, primarily working at the intersection of resource-efficient and human-centered ML. Go Longhorns! <img src="images/texas_longhorns.png" alt="longhorns" width="4%"></li>
      <li> <a href="#HILL22_CALIB">Can Calibration Improve Sample Prioritization?</a> accepted at the  Human in the Loop Learning (HILL) and the Has It Trained Yet? (HITY) workshops @ NeurIPS 2022.</li>
      <!--<li> <a href="#IMWUT22_FL">Federated Learning for Continuous Heterogeneous Sensing</a> to be submitted soon.</li>-->
      <li> <a href="#ICMLA22_AD">Model Selection in Unsupervised Anomaly Detection</a> accepted at IEEE ICMLA 2022. A <a href="#AD_Patent">patent</a> was also filed on the same with automatic threshold optimization.</li>
      <li> Attended the amazing <a href="https://www.eeml.eu/" target="_blank">EEML 2021</a> again! Also, presented my poster/video on <a href="#INTERSPEECH21">Zero-Shot FL with New Classes</a>.</li>
      <li> Presented a talk on <a href="#MOBIUK21">On-Device Zero-Shot FL with New Classes</a> at MobiUK 2021.</li>
      <li> <a href="#INTERSPEECH21">Zero-Shot Federated Learning with New Classes for Audio Classification</a> accepted at INTERSPEECH 2021. Abridged versions accepted at DPML and HAET workshops @ ICLR 2021.</li><!--<a href="#MOBIUK21">MobiUK 2021</a> and EEML 2021.-->
      <li> First time at NeurIPS (virtually)!<br>
        - Two papers – <a href="#MLMH20_STRESS">Bayesian Active Learning for Wearable Stress Detection</a> and <a href="#DLHAR20_HETEROFDL">Federated Learning with Heterogeneous Labels and Models for HAR</a> accepted at ML for Mobile Health Workshop @ NeurIPS 2020.<br>
        - <a href="#ALHEALTH_BDL20">Bayesian Active Learning for Wearable and Mobile Health</a> poster also accepted at NeurIPS Europe meetup on Bayesian Deep Learning 2020.</li>

    <a href="javascript:toggleblock('oldnews')">-- more --</a>
    <div id="oldnews" style="display:none">
      <li> <a href="#patents">Two new patents</a> filed on handling heterogeneous and new labels during federated learning.
      <li> Our project AIB (Automated Intelligent knowledge Base) won Ericsson's Top Performance Competition 2020.</li>
      <li> <a href="https://www.ericsson.com/en/blog/2020/7/how-to-make-anomaly-detection-more-accessible" target="_blank">E-ADF featured on Ericsson blog</a>. E-ADF is our in-house end-to-end framework for anomaly detection.
      <li> <a href="#DLHAR20_HETEROFDL">Resource-Constrained Federated Learning with Heterogeneous Labels and Models for HAR</a> accepted at <br>DL-HAR Workshop @ IJCAI 2020. An <a href="#AIoT20_HETEROFDL">abridged version</a> (with vision) accepted at AIoT Workshop @ KDD 2020.</li>
      <li> Gave a talk on <a href="#talks">Resource-Constrained Machine Learning for Ubiquitous Computing</a> at Flipped by GAIUS.</li>
      <li> Had a great time attending the amazing <a href="https://www.eeml.eu/" target="_blank">EEML 2020</a> and <a href="https://www.oxfordml.school/" target="_blank">OxML 2020</a>! Presented my poster/video on <a href="#EMDL19">ActiveHARNet</a> at EEML, and obtained a full-fee waiver to attend OxML.</li>
      <li> <a href="#ICDMW19">A Dynamic Adaptive Movie Occupancy Forecasting System</a> accepted at LMID Workshop @ IEEE ICDM 2019.</li>
      <li> We won the Shared Task 1 Challenge, Subtask (a) in post-evaluation phase at GermEval @ KONVENS 2019. <a href="#GERMEVAL19">Label Frequency Transformation for Multi-Label Multi-Class Text Classification</a> has been accepted here.</li>
      <li> <a href="#PURBA19">On-Device Intelligent Bus Stop Recognition System</a> accepted at PURBA Workshop @ ACM UbiComp 2019.</li>
      <li> <a href="#EMDL19">ActiveHARNet</a> accepted at Embedded and Mobile Deep Learning Workshop @ ACM MobiSys 2019.</li>
      <li> <a href="#MOBIUK19">Handling Real-time Unlabeled Data in HAR</a> presented at MobiUK 2019, University of Oxford.</li>
      <li> Joined Global AI Accelerator (GAIA) team at Ericsson R&D, broadly working on solving ML problems in telecom.</li>
      <li> <a href="#EMDL18">HARNet</a> accepted at Embedded and Mobile Deep Learning Workshop @ ACM MobiSys 2018.</li>
      <li> Joined SmartCardia (EPFL) as a Machine Learning Engineer, working on AI assisted wearable health-care.</li>
      <li> <a href="#FICC18">Multi-modal Dynamic Gesture Recognition System using Machine Learning</a> accepted at IEEE FICC 2018.</li>
    </div>
    </ul>
  </td></tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr>
    <td>
    <sectionheading id="publications">&nbsp;&nbsp;&nbsp;&nbsp;Publications</sectionheading> &nbsp;&nbsp;&nbsp;[ <a href="#preprints" align="center">Preprints</a> | <a href="#conference" align="center">Conference/Journal/Workshop</a> | <a href="#poster">Poster/Extended Abstract</a> ]<br>
    <center>* denotes equal contribution and joint lead authorship</center>
    </td>
  </tr>
</table>

<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr>
    <td>
    <sectionheading id="publications">&nbsp;&nbsp;Publications</sectionheading><br>
&nbsp;&nbsp;&nbsp;[ <a href="#under_progress" align="center">To be Submitted/Research Under Progress</a> | <a href="#conference" align="center">Conference/Journal/Workshop</a> | <a href="#poster">Poster/Extended Abstract</a> ]<br>
    <center><font size=1>* Equal Contribution</font></center>
    </td>
  </tr>
</table>-->

<!--
<center>
[ <a href="#conference">Conference/Journal/Workshop</a> | <a href="#poster">Poster/Extended Abstracts</a>]
<center><font size=1>* Equal Contribution</font></center>
</center>
<br>
-->

<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><heading id="under_progress">&nbsp;&nbsp;To be Submitted/Research Under Progress</heading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top" align="center"></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Can Explanations Improve Curriculum Learning and Subset Selection?</heading><br>
      <i>Research Under Progress</i><br>
      </p>

      <div class="paper" id="imwut22_fl">
      <a href="javascript:toggleblock('icmla22_ad_abs')">abstract</a>

      <p align="justify"> <i id="icmla22_ad_abs">Anomaly Detection is a widely used technique in machine learning that identifies context-specific outliers. Most real-world anomaly detection applications are unsupervised, owing to the complexity of obtaining labeled data for the given context. In this paper, we solve two important problems pertaining to unsupervised anomaly detection. First, we identify only the most informative subsets of data points to obtain the ground truths from the domain expert (oracle); second, we perform efficient model selection using a Bayesian Inference framework and recommend the top-k models to be fine-tuned prior to deployment. To this end, we exploit multiple existing and novel acquisition functions, and successfully demonstrate the effectiveness of the proposed framework using a weighted Ranking Score (\eta) to accurately rank the top-k models. Our empirical results show a significant reduction in data points acquired (with at least 60% reduction) while not compromising on the efficiency of the top-k models chosen.</i></p>

      </div>
    </td>
  </tr>

</table>-->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><heading id="preprints">&nbsp;&nbsp;Preprints</heading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top" align="center"><img src="images/PCL25.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Unsupervised Latent Partitioning of Foundation Models: Towards Efficient Continual Learning with Experts for Audio Classification</heading>
      <br>
      <b><u>Gautham Krishna Gudur</u></b>*, Mohit Malu*, Reza Rahimi Azghan, Anirudh Rayas, Pavan Turaga, Hassan Ghasemzadeh, Edison Thomaz, Giulia Pedrielli<br>
      </p>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2509.23077" target="_blank"><img src="images/CLADNet25.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <a href="https://arxiv.org/abs/2509.23077" id="CLADNet25" target="_blank">
      <heading>CLAD-Net: Continual Activity Recognition in Multi-Sensor Wearable Systems</heading>
      </a><br>
      Reza Rahimi Azghan, <b><u>Gautham Krishna Gudur</u></b>, Mohit Malu, Edison Thomaz, Giulia Pedrielli, Pavan Turaga, Hassan Ghasemzadeh<br>
      <i>[Submitted]</i>
      </p>

      <div class="paper" id="cladnet25">
        <a href="https://arxiv.org/pdf/2509.23077" target="_blank">pdf</a> /
        <a href="javascript:toggleblock('cladnet25_abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('cladnet25')" class="togglebib">bibtex</a>
  
        <p align="justify"> <i id="cladnet25_abs">The rise of deep learning has greatly advanced human behavior monitoring using wearable sensors, particularly human activity recognition (HAR). While deep models have been widely studied, most assume stationary data distributions - an assumption often violated in real-world scenarios. For example, sensor data from one subject may differ significantly from another, leading to distribution shifts. In continual learning, this shift is framed as a sequence of tasks, each corresponding to a new subject. Such settings suffer from catastrophic forgetting, where prior knowledge deteriorates as new tasks are learned. This challenge is compounded by the scarcity and inconsistency of labeled data in human studies. To address these issues, we propose CLAD-Net (Continual Learning with Attention and Distillation), a framework enabling wearable-sensor models to be updated continuously without sacrificing performance on past tasks. CLAD-Net integrates a self-supervised transformer, acting as long-term memory, with a supervised Convolutional Neural Network (CNN) trained via knowledge distillation for activity classification. The transformer captures global activity patterns through cross-attention across body-mounted sensors, learning generalizable representations without labels. Meanwhile, the CNN leverages knowledge distillation to retain prior knowledge during subject-wise fine-tuning. On PAMAP2, CLAD-Net achieves 91.36 percent final accuracy with only 8.78 percent forgetting, surpassing memory-based and regularization-based baselines such as Experience Replay and Elastic Weight Consolidation. In semi-supervised settings with only 10-20 percent labeled data, CLAD-Net still delivers strong performance, demonstrating robustness to label scarcity. Ablation studies further validate each module's contribution.<i></p>
  
  <pre xml:space="preserve">
  @misc{azghan_cladnet25,
    title={CLAD-Net: Continual Activity Recognition 
    in Multi-Sensor Wearable Systems},
    author={Azghan, Reza Rahimi and Gudur, Gautham 
    Krishna and Malu, Mohit and Thomaz, Edison and 
    Pedrielli, Giulia and Turaga, Pavan and 
    Ghasemzadeh, Hassan},
    year={2025},
    eprint={2509.23077},
    archivePrefix={arXiv}
  }
  </pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2509.18457" target="_blank"><img src="images/GluMind25.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <a href="https://arxiv.org/abs/2509.18457" id="GluMind25" target="_blank">
      <heading>GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting</heading>
      </a><br>
      Ebrahim Farahmand, Reza Rahimi Azghan, Nooshin Taheri Chatrudi, Velarie Yaa Ansu-Baidoo, Eric Kim, <b><u>Gautham Krishna Gudur</u></b>, Mohit Malu, Owen Kruger, Edison Thomaz, Giulia Pedrielli, Pavan Turaga, Hassan Ghasemzadeh<br>
      <i>[Submitted]</i>
      </p>

      <div class="paper" id="glumind25">
        <a href="https://arxiv.org/pdf/2509.18457" target="_blank">pdf</a> /
        <a href="javascript:toggleblock('glumind25_abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('glumind25')" class="togglebib">bibtex</a>
  
        <p align="justify"> <i id="glumind25_abs">Diabetes is a chronic metabolic disorder characterized by persistently high blood glucose levels (BGLs), leading to severe complications such as cardiovascular disease, neuropathy, and retinopathy. Predicting BGLs enables patients to maintain glucose levels within a safe range and allows caregivers to take proactive measures through lifestyle modifications. Continuous Glucose Monitoring (CGM) systems provide real-time tracking, offering a valuable tool for monitoring BGLs. However, accurately forecasting BGLs remains challenging due to fluctuations due to physical activity, diet, and other factors. Recent deep learning models show promise in improving BGL prediction. Nonetheless, forecasting BGLs accurately from multimodal, irregularly sampled data over long prediction horizons remains a challenging research problem. In this paper, we propose AttenGluco, a multimodal Transformer-based framework for long-term blood glucose prediction. AttenGluco employs cross-attention to effectively integrate CGM and activity data, addressing challenges in fusing data with different sampling rates. Moreover, it employs multi-scale attention to capture long-term dependencies in temporal data, enhancing forecasting accuracy. To evaluate the performance of AttenGluco, we conduct forecasting experiments on the recently released AIREADI dataset, analyzing its predictive accuracy across different subject cohorts including healthy individuals, people with prediabetes, and those with type 2 diabetes. Furthermore, we investigate its performance improvements and forgetting behavior as new cohorts are introduced. Our evaluations show that AttenGluco improves all error metrics, such as root mean square error (RMSE), mean absolute error (MAE), and correlation, compared to the multimodal LSTM model. AttenGluco outperforms this baseline model by about 10% and 15% in terms of RMSE and MAE, respectively.This paper proposes GluMind, a transformer-based multimodal framework designed for continual and long-term blood glucose forecasting. GluMind devises two attention mechanisms, including cross-attention and multi-scale attention, which operate in parallel and deliver accurate predictive performance. Cross-attention effectively integrates blood glucose data with other physiological and behavioral signals such as activity, stress, and heart rate, addressing challenges associated with varying sampling rates and their adverse impacts on robust prediction. Moreover, the multi-scale attention mechanism captures long-range temporal dependencies. To mitigate catastrophic forgetting, GluMind incorporates a knowledge retention technique into the transformer-based forecasting model. The knowledge retention module not only enhances the model's ability to retain prior knowledge but also boosts its overall forecasting performance. We evaluate GluMind on the recently released AIREADI dataset, which contains behavioral and physiological data collected from healthy people, individuals with prediabetes, and those with type 2 diabetes. We examine the performance stability and adaptability of GluMind in learning continuously as new patient cohorts are introduced. Experimental results show that GluMind consistently outperforms other state-of-the-art forecasting models, achieving approximately 15% and 9% improvements in root mean squared error (RMSE) and mean absolute error (MAE), respectively.<i></p>
  
  <pre xml:space="preserve">
  @misc{farahmand_glumind25,
    title={GluMind: Multimodal Parallel Attention 
    and Knowledge Retention for Robust 
    Cross-Population Blood Glucose Forecasting}, 
    author={Farahmand, Ebrahim and Azghan, 
    Reza Rahimi and Chatrudi, Nooshin Taheri and 
    Ansu-Baidoo, Velarie Yaa and Kim, Eric and 
    Gudur, Gautham Krishna and Malu, Mohit and 
    Krueger, Owen and Thomaz, Edison and 
    Pedrielli, Giulia and Turaga, Pavan and 
    Ghasemzadeh, Hassan},
    year={2025},
    eprint={2509.18457},
    archivePrefix={arXiv}
  }
  </pre>
  
        </div>
    </td>
  </tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><heading id="conference">&nbsp;&nbsp;Conference/Journal/Workshop</heading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2502.09919" target="_blank"><img src="images/AttenGluco25.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <a href="https://arxiv.org/abs/2502.09919" id="AttenGluco25" target="_blank">
      <heading>AttenGluco: Multimodal Transformer-Based Blood Glucose Forecasting on AI-READI Dataset</heading>
      </a><br>
      Ebrahim Farahmand, Reza Rahimi Azghan, Nooshin Taheri Chatrudi, Eric Kim, <b><u>Gautham Krishna Gudur</u></b>, Edison Thomaz, Giulia Pedrielli, Pavan Turaga, Hassan Ghasemzadeh<br>
      <b>IEEE EMBC 2025</b>
      </p>

      <div class="paper" id="attengluco25">
      <a href="https://arxiv.org/pdf/2502.09919" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('attengluco25_abs')">abstract</a> /
      <a href="https://github.com/rzarhmi/AttenGluco" target="_blank">code</a> /
      <a shape="rect" href="javascript:togglebib('attengluco25')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="attengluco25_abs">Diabetes is a chronic metabolic disorder characterized by persistently high blood glucose levels (BGLs), leading to severe complications such as cardiovascular disease, neuropathy, and retinopathy. Predicting BGLs enables patients to maintain glucose levels within a safe range and allows caregivers to take proactive measures through lifestyle modifications. Continuous Glucose Monitoring (CGM) systems provide real-time tracking, offering a valuable tool for monitoring BGLs. However, accurately forecasting BGLs remains challenging due to fluctuations due to physical activity, diet, and other factors. Recent deep learning models show promise in improving BGL prediction. Nonetheless, forecasting BGLs accurately from multimodal, irregularly sampled data over long prediction horizons remains a challenging research problem. In this paper, we propose AttenGluco, a multimodal Transformer-based framework for long-term blood glucose prediction. AttenGluco employs cross-attention to effectively integrate CGM and activity data, addressing challenges in fusing data with different sampling rates. Moreover, it employs multi-scale attention to capture long-term dependencies in temporal data, enhancing forecasting accuracy. To evaluate the performance of AttenGluco, we conduct forecasting experiments on the recently released AIREADI dataset, analyzing its predictive accuracy across different subject cohorts including healthy individuals, people with prediabetes, and those with type 2 diabetes. Furthermore, we investigate its performance improvements and forgetting behavior as new cohorts are introduced. Our evaluations show that AttenGluco improves all error metrics, such as root mean square error (RMSE), mean absolute error (MAE), and correlation, compared to the multimodal LSTM model. AttenGluco outperforms this baseline model by about 10% and 15% in terms of RMSE and MAE, respectively.</i></p>

<pre xml:space="preserve">
@article{farahmand_attengluco25,
  author = {Farahmand, Ebrahim and Azghan, 
  Reza Rahimi and Chatrudi, Nooshin Taheri,
  and Kim, Eric and Gudur, Gautham Krishna
  and Thomaz, Edison and Pedrielli, Giulia
  and Turaga, Pavan and Ghasemzadeh, Hassan}
  title = {AttenGluco: Multimodal 
  Transformer-Based Blood Glucose Forecasting
  on AI-READI Dataset},
  journal = {arXiv preprint arXiv:2502.09919},
  year = {2025}
}
</pre>

      </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center"><a href="https://neurips.cc/virtual/2024/poster/93272" target="_blank"><img src="images/SVFT24.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <a href="https://neurips.cc/virtual/2024/poster/93272" id="SVFT24" target="_blank">
      <heading>SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors</heading>
      </a><br>
      Vijay Lingam*, Atula Tejaswi*, Aditya Vavre*, Aneesh Shetty*, <b><u>Gautham Krishna Gudur</u></b>*, Joydeep Ghosh, Alex Dimakis, Eunsol Choi, Aleksandar Bojchevski, Sujay Sanghavi<br>
      <b>Neural Information Processing Systems (NeurIPS), 2024</b><br>
      <a href="https://openreview.net/forum?id=DOUskwCqg5" target="_blank">Abridged version</a>: Fine-Tuning in Modern Machine Learning: Principles and Scalability (FITML) workshop, <b>NeurIPS 2024</b><br>
      Workshop on Advancing Neural Network Training (WANT): Computational Efficiency, Scalability, and Resource Optimization <b><i>[Oral Presentation]</i></b><br>
      Workshop on Efficient Systems for Foundation Models (ES-FoMo)<br>
      <b>ICML 2024</b>
      </p>
  
      <div class="paper" id="svft24">
      <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/48c368f105e8145b945227b73255635a-Paper-Conference.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('svft24_abs')">abstract</a> /
      <a href="publications/SVFT_WANT_ICML24_Poster.pdf" target="_blank">poster</a> /
      <a href="https://github.com/VijayLingam95/SVFT" target="_blank">code</a> /
      <a href="https://x.com/atu_tej/status/1799519888588837273" target="_blank">tweet</a> /
      <a shape="rect" href="javascript:togglebib('svft24')" class="togglebib">bibtex</a>
  
      <p align="justify"> <i id="svft24_abs">Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights <b>W</b> and inject learnable matrices <b>∆W</b>. These <b>∆W</b> matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically exhibit a performance gap compared to full fine-tuning. While recent PEFT methods have narrowed this gap, they do so at the expense of additional learnable parameters. We propose SVFT, a simple approach that structures <b>∆W</b> based on the specific weight matrix <b>W</b>. SVFT updates <b>W</b> as a sparse combination M of outer products of its singular vectors, training only the coefficients of these combinations. Crucially, we make additional off-diagonal elements in M learnable, enabling a smooth trade-off between trainable parameters and expressivity — an aspect that distinctly sets our approach apart from previous works leveraging singular values. Extensive experiments on language and vision benchmarks show that SVFT recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25% of parameters, outperforming existing methods that achieve only up to 85% performance with 0.03 to 0.8% of the trainable parameter budget.</i></p>
  
  <pre xml:space="preserve">
  @inproceedings{lingam_neurips24,
    title = {SVFT: Parameter-Efficient Fine-Tuning
    with Singular Vectors},
    author = {Lingam, Vijay and Tejaswi, Atula
    and Vavre, Aditya and Shetty, Aneesh
    and Gudur, Gautham Krishna and Ghosh, Joydeep
    and Dimakis, Alex and Choi, Eunsol and
    Bojchevski, Aleksandar and Sanghavi, Sujay},
    pages = {41425--41446},
    booktitle = {Advances in Neural Information 
    Processing Systems},
    year = {2024}
  }
  </pre>
  
      </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center"><a href="https://neurips2024-enlsp.github.io/papers/paper_91.pdf" target="_blank"><img src="images/ENLSP24.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <a href="https://neurips2024-enlsp.github.io/papers/paper_91.pdf" id="ENLSP24_NeurIPS" target="_blank">
      <heading>Dataset Distillation for Audio Classification: A Data-Efficient Alternative to Active Learning</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Edison Thomaz<br>
      Efficient Natural Language and Speech Processing (ENLSP) Workshop<br>
      <b>NeurIPS 2024</b>
      </p>

      <div class="paper" id="enlsp24">
      <a href="https://neurips2024-enlsp.github.io/papers/paper_91.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('enlsp24_neurips_abs')">abstract</a> /
      <a href="publications/DD_ENLSP_NeurIPS24.pdf" target="_blank">poster</a> /
      <a shape="rect" href="javascript:togglebib('enlsp24')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="enlsp24_neurips_abs">Audio classification tasks like keyword spotting and acoustic event detection often require large labeled datasets, which are computationally expensive and impractical for resource-constrained devices. While active learning techniques attempt to reduce labeling efforts by selecting the most informative samples, they struggle with scalability in real-world scenarios involving thousands of audio segments. In this paper, we introduce an approach that leverages dataset distillation as an alternative strategy to active learning to address the challenge of data efficiency in real-world audio classification tasks. Our approach synthesizes compact, high-fidelity coresets that encapsulate the most critical information from the original dataset, significantly reducing the labeling requirements while offering competitive performance. Through experiments on three benchmark datasets -- Google Speech Commands, UrbanSound8K, and ESC-50, our approach achieves up to a ~3,000x reduction in data points, and requires only a negligible fraction of the original training data while matching the performance of popular active learning baselines.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_enlsp24,
  author = {Gudur, Gautham Krishna and
  and Thomaz, Edison},
  title = {Dataset Distillation for Audio 
  Classification: A Data-Efficient 
  Alternative to Active Learning},
  booktitle = {NeurIPS 2024 Efficient 
  Natural Language and Speech Processing 
  (ENLSP-IV) Workshop},
  year = {2024}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2210.06592" target="_blank"><img src="images/HILL22.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <a href="https://arxiv.org/abs/2210.06592" id="HILL22_CALIB" target="_blank">
      <heading>Can Calibration Improve Sample Prioritization?</heading>
      </a><br>
      Ganesh Tata*, <b><u>Gautham Krishna Gudur</u></b>*, Gopinath Chennupati, Mohammad Emtiyaz Khan<br>
      Human in the Loop Learning (HILL) Workshop<br>
      Has It Trained Yet? (HITY) Workshop<br><b>NeurIPS 2022</b>
      </p>

      <div class="paper" id="hill22_calib">
      <a href="https://openreview.net/pdf?id=LnygZu8WJk" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('hill22_calib_abs')">abstract</a> /
      <a href="publications/Calib_NeurIPS22_Poster.pdf" target="_blank">poster</a> / 
      <a href="https://github.com/tataganesh/Calib-Sample-Prioritization" target="_blank">code</a> /
      <a shape="rect" href="javascript:togglebib('hill22_calib')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="hill22_calib_abs">Calibration can reduce overconfident predictions of deep neural networks, but can calibration also accelerate training? In this paper, we show that it can when used to prioritize some examples for performing subset selection. We study the effect of popular calibration techniques in selecting better subsets of samples during training (also called sample prioritization) and observe that calibration can improve the quality of subsets, reduce the number of examples per epoch (by at least 70%), and can thereby speed up the overall training process. We further study the effect of using calibrated pre-trained models coupled with calibration during training to guide sample prioritization, which again seems to improve the quality of samples selected.</i></p>

<pre xml:space="preserve">
@inproceedings{tata_hity22,
  author = {Tata, Ganesh and Gudur, Gautham Krishna
  and Chennupati, Gopinath and 
  Khan, Mohammad Emtiyaz},
  title = {Can Calibration Improve Sample 
  Prioritization?},
  booktitle = {Has it Trained Yet? 
  NeurIPS 2022 Workshop},
  year = {2022}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://ieeexplore.ieee.org/document/10069418" target="_blank"><img src="images/ICMLA22.png" alt="sym" width="100%" style="border-radius:15px;"></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <a href="https://ieeexplore.ieee.org/document/10069418" id="ICMLA22_AD" target="_blank">
      <heading>Data-Efficient Automatic Model Selection in Unsupervised Anomaly Detection</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Raaghul R, Adithya K, Shrihari Vasudevan<br>
      <b>IEEE ICMLA 2022</b><br>
      </p>

      <div class="paper" id="icmla22_ad">
      <a href="publications/BayesianAD_ICMLA22.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('icmla22_ad_abs')">abstract</a> /
      <a href="slides/ICMLA22_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('icmla22_ad')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="icmla22_ad_abs">Anomaly Detection is a widely used technique in machine learning that identifies context-specific outliers. Most real-world anomaly detection applications are unsupervised, owing to the bottleneck of obtaining labeled data for a given context. In this paper, we solve two important problems pertaining to unsupervised anomaly detection. First, we identify only the most informative subsets of data points and obtain ground truths from the domain expert (oracle); second, we perform efficient model selection using a Bayesian Inference framework and recommend the top-k models to be fine-tuned prior to deployment. To this end, we exploit multiple existing and novel acquisition functions, and successfully demonstrate the effectiveness of the proposed framework using a weighted Ranking Score (\eta) to accurately rank the top-k models. Our empirical results show a significant reduction in data points acquired (with at least 60% reduction) while not compromising on the efficiency of the top-k models chosen, with both uniform and non-uniform priors over models.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_icmla22,
  author = {Gudur, Gautham Krishna and Raaghul, R
  and Adithya, K and Vasudevan, Shrihari},
  title = {Data-Efficient Automatic Model 
  Selection in Unsupervised Anomaly Detection},
  booktitle = {IEEE ICMLA 2022},
  year = {2022}
}
</pre>

      </div>
    </td>
  </tr>
  
  <tr>
    <td width="33%" valign="top" align="center"><a href="https://www.isca-speech.org/archive/interspeech_2021/gudur21_interspeech.html" target="_blank"><img src="images/INTERSPEECH21.png" alt="sym" width="65%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <a href="https://www.isca-archive.org/interspeech_2021/gudur21_interspeech.html" id="INTERSPEECH21" target="_blank">
      <heading>Zero-Shot Federated Learning with New Classes for Audio Classification</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Satheesh Kumar Perepu<br>
      <b>INTERSPEECH 2021</b><br>
      <a href="https://dp-ml.github.io/2021-workshop-ICLR/files/44.pdf" target="_blank">Abridged version</a>: Distributed and Private Machine Learning (DPML) & Hardware Aware Efficient Training (HAET) workshops, <b>ICLR 2021</b><br>
      Also presented as a <a href="https://www.youtube.com/watch?v=U_aWp6wajzM" target="_blank">poster</a> at <b>EEML 2021</b>
      </p>

      <div class="paper" id="interspeech21_newclassfl">
      <a href="https://www.isca-archive.org/interspeech_2021/gudur21_interspeech.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('interspeech21_newclassfl_abs')">abstract</a> /
      <a href="publications/NewClassFL_ICLR21_Poster.pdf" target="_blank">poster</a> /
      <a href="https://www.youtube.com/watch?v=U_aWp6wajzM" target="_blank">video</a> /
      <a href="slides/DPML21_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('interspeech21_newclassfl')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="interspeech21_newclassfl_abs">Federated learning is an effective way of extracting insights from different user devices while preserving the privacy of users. However, new classes with completely unseen data distributions can stream across any device in a federated learning setting, whose data cannot be accessed by the global server or other users. To this end, we propose a unified zero-shot framework to handle these aforementioned challenges during federated learning. We simulate two scenarios here – 1) when the new class labels are not reported by the user, the traditional FL setting is used; 2) when new class labels are reported by the user, we synthesize Anonymized Data Impressions by calculating class similarity matrices corresponding to each device’s new classes followed by unsupervised clustering to distinguish between new classes across different users. Moreover, our proposed framework can also handle statistical heterogeneities in both labels and models across the participating users. We empirically evaluate our framework on-device across different communication rounds (FL iterations) with new classes in both local and global updates, along with heterogeneous labels and models, on two widely used audio classification applications – keyword spotting and urban sound classification, and observe an average deterministic accuracy increase of ∼4.041% and ∼4.258% respectively.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_interspeech21,
  author = {Gudur, Gautham Krishna and 
  Perepu, Satheesh Kumar},
  title = {Zero-Shot Federated Learning with 
  New Classes for Audio Classification},
  booktitle = {Proc. Interspeech 2021},
  pages = {1579--1583},
  year = {2021},
  doi = {10.21437/Interspeech.2021-2264}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2012.02702" target="_blank"><img src="images/MLMH20_Stress.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://arxiv.org/abs/2012.02702" id="MLMH20_STRESS" target="_blank">
      <heading>Bayesian Active Learning for Wearable Stress and Affect Detection</heading>
      </a><br>
      Abhijith Ragav*, <b><u>Gautham Krishna Gudur</u></b>*<br>
      Machine Learning for Mobile Health Workshop<br><b>NeurIPS 2020</b>
      </p>

      <div class="paper" id="mlmh20_stress">
      <a href="https://arxiv.org/pdf/2012.02702.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('mlmh20_stress_abs')">abstract</a> /
      <a href="publications/ALStress_MLMH_NeurIPS20_Poster.pdf" target="_blank">poster</a> /
      <a shape="rect" href="javascript:togglebib('mlmh20_stress')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="mlmh20_stress_abs">In the recent past, psychological stress has been increasingly observed in humans, and early detection is crucial to prevent health risks. Stress detection using ondevice deep learning algorithms has been on the rise owing to advancements in pervasive computing. However, an important challenge that needs to be addressed is handling unlabeled data in real-time via suitable ground truthing techniques (like Active Learning), which should help establish affective states (labels) while also selecting only the most informative data points to query from an oracle. In this paper, we propose a framework with capabilities to represent model uncertainties through approximations in Bayesian Neural Networks using Monte-Carlo (MC) Dropout. This is combined with suitable acquisition functions for active learning. Empirical results on a popular stress and affect detection dataset experimented on a Raspberry Pi 2 indicate that our proposed framework achieves a considerable efficiency boost during inference, with a substantially low number of acquired pool points during active learning across various acquisition functions. Variation Ratios achieves an accuracy of 90.38% which is comparable to the maximum test accuracy achieved while training on about 40% lesser data.</i></p>

<pre xml:space="preserve">
@article{ragav_mlmh20,
  author = {Ragav, Abhijith and 
  Gudur, Gautham Krishna},
  title = {Bayesian Active Learning for 
  Wearable Stress and Affect Detection},
  journal = {arXiv preprint arXiv:2012.02702},
  year = {2020}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://link.springer.com/chapter/10.1007/978-981-16-0575-8_5" target="_blank"><img src="images/DLHAR20.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://link.springer.com/chapter/10.1007/978-981-16-0575-8_5" id="DLHAR20_HETEROFDL" target="_blank">
      <heading>Resource-Constrained Federated Learning with Heterogeneous Labels and Models for Human Activity Recognition</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Satheesh Kumar Perepu<br>
      2nd International Workshop on Deep Learning for Human Activity Recognition (DL-HAR), <b>IJCAI-PRICAI 2020</b><br>
      <a href="https://arxiv.org/pdf/2012.02539.pdf" target="_blank">Abridged version</a>: Machine Learning for Mobile Health Workshop,<br> <b>NeurIPS 2020</b>
      </p>

      <div class="paper" id="dlhar20_heterofdl">
      <a href="publications/HeteroFDL_DLHAR_IJCAI20.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('dlhar20_heterofdl_abs')">abstract</a> /
      <a href="publications/HeteroFDL_MLMH_NeurIPS20_Poster.pdf" target="_blank">poster</a> /
      <a href="slides/DLHAR20_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('dlhar20_heterofdl')" class="togglebib">bibtex</a>
      <br>
      <p align="justify"> <i id="dlhar20_heterofdl_abs">One of the most significant applications in pervasive computing for modeling user behavior is Human Activity Recognition (HAR). Such applications necessitate us to characterize insights from multiple resource-constrained user devices using machine learning techniques for effective personalized activity monitoring. On-device Federated Learning proves to be an extremely viable option for distributed and collaborative machine learning in such scenarios, and is an active area of research. However, there are a variety of challenges in addressing statistical (non-IID data) and model heterogeneities across users. In addition, in this paper, we explore a new challenge of interest - to handle heterogeneities in labels (activities) across users during federated learning. To this end, we propose a framework with two different versions for federated label-based aggregation, which leverage overlapping information gain across activities - one using Model Distillation Update, and the other using Weighted \alpha$-update. Empirical evaluation on the Heterogeneity Human Activity Recognition (HHAR) dataset (with four activities for effective elucidation of results) indicates an average deterministic accuracy increase of at least ~11.01% with the model distillation update strategy and ~9.16% with the weighted \alpha-update strategy. We demonstrate the on-device capabilities of our proposed framework by using Raspberry Pi 2, a single-board computing platform.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_dlhar20,
  author = {Gudur, Gautham Krishna and 
  Perepu, Satheesh K},
  title = {Resource-Constrained Federated Learning
  with Heterogeneous Labels and Models for 
  Human Activity Recognition},
  booktitle = {Deep Learning for Human Activity 
  Recognition},
  pages = {55--69},
  year = {2021},
  publisher={Springer Singapore}
}

@article{gudur_mlmh20,
  author = {Gudur, Gautham Krishna and
  Perepu, Satheesh K},
  title = {Federated Learning with Heterogeneous 
  Labels and Models for Mobile Activity 
  Monitoring},
  journal = {arXiv preprint arXiv:2012.02539},
  year = {2020}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2011.03206" target="_blank"><img src="images/AIoT20.png" alt="sym" width="75%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://arxiv.org/abs/2011.03206" id="AIoT20_HETEROFDL" target="_blank">
      <heading>Resource-Constrained Federated Learning with Heterogeneous Labels and Models</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Bala Shyamala Balaji, Satheesh Kumar Perepu<br>
      3rd International Workshop on Artificial Intelligence of Things (AIoT)<br><b>ACM KDD 2020</b>
      </p>

      <div class="paper" id="aiot20_heterofdl">
      <a href="https://aiotworkshop.github.io/published/AIoT_10_Gudur_TechnicalPaper_KDD2020.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('aiot20_heterofdl_abs')">abstract</a> /
      <a href="slides/AIoT20_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('aiot20_heterofdl')" class="togglebib">bibtex</a>
      <br>
      <p align="justify"> <i id="aiot20_heterofdl_abs">Various IoT applications demand resource-constrained machine learning mechanisms for different applications such as pervasive healthcare, activity monitoring, speech recognition, real-time computer vision, etc. This necessitates us to leverage information from multiple devices with few communication overheads. Federated Learning proves to be an extremely viable option for distributed and collaborative machine learning. Particularly, on-device federated learning is an active area of research, however, there are a variety of challenges in addressing statistical (non-IID data) and model heterogeneities. In addition, in this paper we explore a new challenge of interest - to handle label heterogeneities in federated learning. To this end, we propose a framework with simple $\alpha$-weighted federated aggregation of scores which leverages overlapping information gain across labels, while saving bandwidth costs in the process. Empirical evaluation on Animals-10 dataset (with 4 labels for effective elucidation of results) indicates an average deterministic accuracy increase of at least ~16.7%. We also demonstrate the on-device capabilities of our proposed framework by experimenting with federated learning and inference across different iterations on a Raspberry Pi 2, a single-board computing platform.</i></p>

<pre xml:space="preserve">
@article{gudur_aiot20,
  author = {Gudur, Gautham Krishna and Balaji,
  Bala Shyamala and Perepu, Satheesh K},
  title = {Resource-Constrained Federated Learning
  with Heterogeneous Labels and Models},
  journal = {arXiv preprint arXiv:2011.03206},
  year = {2020}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://ieeexplore.ieee.org/document/8955583" target="_blank"><img src="images/ICDMW19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://ieeexplore.ieee.org/document/8955583" id="ICDMW19" target="_blank">
      <heading>A Dynamically Adaptive Movie Occupancy Forecasting System with Feature Optimization</heading>
      </a><br>
      Sundararaman Venkataramani, Ateendra Ramesh, Sharan Sundar S, Aashish Kumar Jain, <b><u>Gautham Krishna Gudur</u></b>, Vineeth Vijayaraghavan<br>
      Workshop on Learning and Mining with Industrial Data (LMID)<br><b>IEEE ICDM 2019</b>
      </p>

      <div class="paper" id="icdmw19">
      <a href="publications/LMID_ICDM19.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('icdmw19_abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('icdmw19')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="icdmw19_abs">Demand Forecasting is a primary revenue management strategy in any business model, particularly in the highly volatile entertainment/movie industry wherein, inaccurate forecasting may lead to loss in revenue, improper workforce allocation and food wastage or shortage. Predominant challenges in Occupancy Forecasting might involve complexities in modeling external factors – particularly in Indian multiplexes with multilingual movies, high degrees of uncertainty in crowdbehavior, seasonality drifts, influence of socio-economic events and weather conditions. In this paper, we investigate the problem of movie occupancy forecasting, a significant step in the decision making process of movie scheduling and resource management, by leveraging the historical transactions performed in a multiplex consisting of eight screens with an average footfall of over 5500 on holidays and over 3500 on nonholidays every day. To effectively capture crowd behavior and predict the occupancy, we engineer and benchmark behavioral features by structuring recent historical transaction data spanning over five years from one of the top Indian movie multiplex chains, and propose various deep learning and conventional machine learning models. We also propose and optimize on a novel feature called Sale Velocity to incorporate the dynamic crowd behavior in movies. The performance of these models are benchmarked in real-time using Mean Absolute Percentage Error (MAPE), and found to be highly promising while substantially outperforming a domain expert’s predictions.</i></p>

<pre xml:space="preserve">
@inproceedings{venkataramani_icdmw19,
  author = {Venkataramani, Sundararaman and 
  Ramesh, Ateendra and S, Sharan Sundar and 
  Jain, Aashish Kumar and Gudur, Gautham Krishna
  and Vijayaraghavan, Vineeth},
  title = {A Dynamically Adaptive Movie Occupancy 
  Forecasting System with Feature Optimization},
  booktitle = {International Conference on Data 
  Mining Workshops (ICDMW)},
  pages = {799--805},
  year = {2019},
  organization = {IEEE}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dl.acm.org/doi/10.1145/3341162.3349323" target="_blank"><img src="images/PURBA19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://dl.acm.org/doi/10.1145/3341162.3349323" id="PURBA19" target="_blank">
      <heading>A Vision-based Deep On-Device Intelligent Bus Stop Recognition System</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Ateendra Ramesh, Srinivasan R<br>
      8th International Workshop on Pervasive Urban Applications (PURBA)<br><b>ACM UbiComp 2019</b>
      </p>

      <div class="paper" id="purba19">
      <a href="https://cpemis.eng.cmu.ac.th/~santi/purba2019/papers/p23.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('purba19_abs')">abstract</a> /
      <a href="https://github.com/gauthamkrishna-g/Intelligent-Bus-Stop-Recognition-System" target="_blank">code</a> /
      <a href="slides/PURBA19_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('purba19')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="purba19_abs">Intelligent public transportation systems are the cornerstone to any smart city, given the advancements made in the field of self-driving autonomous vehicles - particularly for autonomous buses, where it becomes really difficult to systematize a way to identify the arrival of a bus stop on-the-fly for the bus to appropriately halt and notify its passengers. This paper proposes an automatic and intelligent bus stop recognition system built on computer vision techniques, deployed on a low-cost single-board computing platform with minimal human supervision. The on-device recognition engine aims to extract the features of a bus stop and its surrounding environment, which eliminates the need for a conventional Global Positioning System (GPS) look-up, thereby alleviating network latency and accuracy issues. The dataset proposed in this paper consists of images of 11 different bus stops taken at different locations in Chennai, India during day and night. The core engine consists of a convolutional neural network (CNN) of size ~260 kB that is computationally lightweight for training and inference. In order to automatically scale and adapt to the dynamic landscape of bus stops over time, incremental learning (model updation) techniques were explored on-device from real-time incoming data points. Real-time incoming streams of images are unlabeled, hence suitable ground truthing strategies (like Active Learning), should help establish labels on-the-fly. Light-weight Bayesian Active Learning strategies using Bayesian Neural Networks using dropout (capable of representing model uncertainties) enable selection of the most informative images to query from an oracle. Intelligent rendering of the inference module by iteratively looking for better images on either sides of the bus stop environment propels the system towards human-like behavior. The proposed work can be integrated seamlessly into the widespread existing vision-based self-driving autonomous vehicles.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_purba19,
  author = {Gudur, Gautham Krishna and Ramesh, 
  Ateendra and R, Srinivasan},
  title = {A Vision-Based Deep On-Device 
  Intelligent Bus Stop Recognition System},
  booktitle = {Adjunct Proceedings of the 2019 ACM  
  International Joint Conference on Pervasive and
  Ubiquitous Computing and Proceedings of the 2019 
  ACM International Symposium on Wearable Computers},
  pages = {963--968},
  numpages = {6},
  year = {2019}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://competitions.codalab.org/competitions/20139" target="_blank"><img src="images/GermEval19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://competitions.codalab.org/competitions/20139" id="GERMEVAL19" target="_blank">
      <heading>Label Frequency Transformation for Multi-Label Multi-Class Text Classification</heading>
      </a><br>
      Raghavan A K, Venkatesh Umaashankar, <b><u>Gautham Krishna Gudur</u></b><br>
      GermEval, <b>KONVENS 2019</b><br>
      (<b>Winner</b> of Shared Task 1, Subtask (a) in post-evaluation phase)
      </p>

      <div class="paper" id="germeval19">
      <a href="https://konvens.org/proceedings/2019/papers/germeval/Germeval_Task1_paper_8.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('germeval19_abs')">abstract</a> /
      <a href="https://gauthamkrishna-g.github.io/publications/GermEval_KONVENS19_Poster.pdf" target="_blank">poster</a> /
      <a href="https://github.com/oneraghavan/germeval-2019" target="_blank">code</a> /
      <a shape="rect" href="javascript:togglebib('germeval19')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="germeval19_abs">In this paper, we (Team Raghavan) describe the system of our submission for GermEval 2019 Task 1 - Subtask (a) and Subtask (b), which are multi-label multi-class classification tasks. The goal is to classify short texts describing German books into one or multiple classes, 8 generic categories for Subtask (a) and 343 specific categories for Subtask (b). Our system comprises of three stages. (a) Transform multi-label multi-class problem into single-label multi-class problem. Build a category model. (b) Build a class count model to predict the number of classes a given input belongs to. (c) Transform single-label problem into multi-label problem back again by selecting the top-k predictions from the category model, with the optimal k value predicted from the class count model. Our approach utilizes a Support Vector Classification model on the extracted vectorized tf-idf features by leveraging the Byte-Pair Token Encoding (BPE), and reaches f1-micro scores of 0.857 in the test evaluation phase and 0.878 in post evaluation phase for Subtask (a), while 0.395 in post evaluation phase for Subtask (b) of the competition. We have provided our solution code in the following link: https://github.com/oneraghavan/germeval-2019.</i></p>

<pre xml:space="preserve">
@inproceedings{raghavan_konvens19,
  author = {Raghavan, AK and Umaashankar, Venkatesh
  and Gudur, Gautham Krishna},
  title = {Label Frequency Transformation for 
  Multi-Label Multi-Class Text Classification},
  booktitle = {Proceedings of the 15th Conference on 
  Natural Language Processing (KONVENS 2019)},
  pages = {341--346},
  year = {2019},
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dl.acm.org/doi/10.1145/3325413.3329790" target="_blank"><img src="images/EMDL19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://dl.acm.org/doi/10.1145/3325413.3329790" id="EMDL19" target="_blank">
      <heading>ActiveHARNet: Towards On-Device Deep Bayesian Active Learning for Human Activity Recognition</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Prahalathan Sundaramoorthy, Venkatesh Umaashankar<br>
      3rd International Workshop on Embedded and Mobile Deep Learning (EMDL), <b>ACM MobiSys 2019</b><br>
      Also presented as a <a href="https://www.youtube.com/watch?v=Kfy0URcPxyE&t" target="_blank">poster</a> at <b>EEML 2020</b>
      </p>

      <div class="paper" id="emdl19">
      <a href="https://arxiv.org/pdf/1906.00108.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('emdl19_abs')">abstract</a> /
      <a href="https://github.com/gauthamkrishna-g/ActiveHARNet" target="_blank">code</a> /
      <a href="https://www.youtube.com/watch?v=Kfy0URcPxyE&t" target="_blank"">video</a> /
      <a href="slides/EMDL19_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('emdl19')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="emdl19_abs">Various health-care applications such as assisted living, fall detection etc., require modeling of user behavior through Human Activity Recognition (HAR). HAR using mobile- and wearable-based deep learning algorithms have been on the rise owing to the advancements in pervasive computing. However, there are two other challenges that need to be addressed: first, the deep learning model should support on-device incremental training (model updation) from real-time incoming data points to learn user behavior over time, while also being resource-friendly; second, a suitable ground truthing technique (like Active Learning) should help establish labels on-the-fly while also selecting only the most informative data points to query from an oracle. Hence, in this paper, we propose ActiveHARNet, a resource-efficient deep ensembled model which supports on-device Incremental Learning and inference, with capabilities to represent model uncertainties through approximations in Bayesian Neural Networks using dropout. This is combined with suitable acquisition functions for active learning. Empirical results on two publicly available wrist-worn HAR and fall detection datasets indicate that ActiveHARNet achieves considerable efficiency boost during inference across different users, with a substantially low number of acquired pool points (at least 60% reduction) during incremental learning on both datasets experimented with various acquisition functions, thus demonstrating deployment and Incremental Learning feasibility.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_emdl19,
  author = {Gudur, Gautham Krishna and 
  Sundaramoorthy, Prahalathan and 
  Umaashankar, Venkatesh},
  title = {ActiveHARNet: Towards On-Device 
  Deep Bayesian Active Learning for Human 
  Activity Recognition},
  booktitle = {The 3rd International Workshop 
  on Deep Learning for Mobile Systems and 
  Applications},
  pages = {7--12},
  numpages = {6},
  year = {2019}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dl.acm.org/doi/10.1145/3212725.3212728" target="_blank"><img src="images/EMDL18.png" alt="sym" width="75%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://dl.acm.org/doi/10.1145/3212725.3212728" id="EMDL18" target="_blank">
      <heading>HARNet: Towards On-Device Incremental Learning using Deep Ensembles on Constrained Devices</heading>
      </a><br>
      Prahalathan Sundaramoorthy, <b><u>Gautham Krishna Gudur</u></b>, Manav Rajiv Moorthy, R Nidhi Bhandari, Vineeth Vijayaraghavan<br>
      2nd International Workshop on Embedded and Mobile Deep Learning (EMDL), <b>ACM MobiSys 2018</b>
      </p>

      <div class="paper" id="emdl18">
      <a href="https://www.sigmobile.org/mobisys/2018/workshops/deepmobile18/papers/HARNet.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('emdl18_abs')">abstract</a> /
      <a href="https://github.com/gauthamkrishna-g/HARNet" target="_blank">code</a> /
      <a href="slides/EMDL18_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('emdl18')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="emdl18_abs">Recent advancements in the domain of pervasive computing have seen the incorporation of sensor-based Deep Learning algorithms in Human Activity Recognition (HAR). Contemporary Deep Learning models are engineered to alleviate the difficulties posed by conventional Machine Learning algorithms which require extensive domain knowledge to obtain heuristic hand-crafted features. Upon training and deployment of these Deep Learning models on ubiquitous mobile/embedded devices, it must be ensured that the model adheres to their computation and memory limitations, in addition to addressing the various mobile- and user-based heterogeneities prevalent in actuality. To handle this, we propose HARNet - a resource-efficient and computationally viable network to enable on-line Incremental Learning and User Adaptability as a mitigation technique for anomalous user behavior in HAR. Heterogeneity Activity Recognition Dataset was used to evaluate HARNet and other proposed variants by utilizing acceleration data acquired from diverse mobile platforms across three different modes from a practical application perspective. We perform Decimation as a Down-sampling technique for generalizing sampling frequencies across mobile devices, and Discrete Wavelet Transform for preserving information across frequency and time. Systematic evaluation of HARNet on User Adaptability yields an increase in accuracy by ~35% by leveraging the model's capability to extract discriminative features across activities in heterogeneous environments.</i></p>

<pre xml:space="preserve">
@inproceedings{sundaramoorthy_emdl18,
  author = {Sundaramoorthy, Prahalathan and 
  Gudur, Gautham Krishna and Moorthy, 
  Manav Rajiv and Bhandari, R Nidhi and 
  Vijayaraghavan, Vineeth},
  title = {HARNet: Towards On-Device 
  Incremental Learning Using Deep Ensembles 
  on Constrained Devices},
  booktitle = {Proceedings of the 2nd International 
  Workshop on Embedded and Mobile Deep Learning},
  pages = {31--36},
  numpages = {6}
  year = {2018}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://link.springer.com/chapter/10.1007/978-3-030-03405-4_42" target="_blank"><img src="images/FICC18.png" alt="sym" width="75%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-03405-4_42" id="FICC18" target="_blank">
      <heading>A Generic Multi-modal Dynamic Gesture Recognition System using Machine Learning</heading>
      </a><br>
      <b><u>Gautham Krishna G</u></b>, Karthik Subramanian Nathan, Yogesh Kumar B, Ankith A Prabhu, Ajay Kannan, Vineeth Vijayaraghavan<br>
      <b>IEEE FICC 2018</b>
      </p>

      <div class="paper" id="ficc18">
      <a href="https://arxiv.org/pdf/1809.05839.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('ficc18_abs')">abstract</a> /
      <a href="https://github.com/gauthamkrishna-g/Dynamic-Gesture-Recognition" target="_blank">code</a> /
      <a href="slides/FICC18_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('ficc18')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="ficc18_abs">Human computer interaction facilitates intelligent communication between humans and computers, in which gesture recognition plays a prominent role. This paper proposes a machine learning system to identify dynamic gestures using triaxial acceleration data acquired from two public datasets. These datasets, uWave and Sony, were acquired using accelerometers embedded in Wii remotes and smartwatches, respectively. A dynamic gesture signed by the user is characterized by a generic set of features extracted across time and frequency domains. The system was analyzed from an end-user perspective and was modelled to operate in three modes. The modes of operation determine the subsets of data to be used for training and testing the system. From an initial set of seven classifiers, three were chosen to evaluate each dataset across all modes rendering the system towards mode-neutrality and dataset-independence. The proposed system is able to classify gestures performed at varying speeds with minimum preprocessing, making it computationally efficient. Moreover, this system was found to run on a low-cost embedded platform – Raspberry Pi Zero (USD 5), making it economically viable.</i></p>

<pre xml:space="preserve">
@inproceedings{krishna_ficc18,
  author = {Krishna, G Gautham and Nathan,
  Karthik Subramanian and Kumar, B Yogesh 
  and Prabhu, Ankith A and Kannan, Ajay and
  Vijayaraghavan, Vineeth},
  title = {A Generic Multi-modal Dynamic Gesture
  Recognition System Using Machine Learning},
  booktitle = {Future of Information and 
  Communication Conference},
  pages = {603--615},
  year = {2018},
  organization = {Springer}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://link.springer.com/chapter/10.1007/978-981-10-5780-9_13" target="_blank"><img src="images/ICAICR17.jpeg" alt="sym" width="50%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://link.springer.com/chapter/10.1007/978-981-10-5780-9_13" id="EEG_ICAICR17" target="_blank">
      <heading>Electroencephalography Based Analysis of Emotions Among Indian Film Viewers</heading>
      </a><br>
      <b><u>Gautham Krishna G</u></b>, G Krishna, N Bhalaji<br>
      <b>ICAICR 2017</b>, Springer
      </p>

      <div class="paper" id="eeg_icaicr17">
      <a href="publications/Neurocinematics_ICAICR17.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('eeg_icaicr17_abs')">abstract</a> /
      <a href="slides/ICAICR17_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('eeg_icaicr17')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="eeg_icaicr17_abs">The film industry has been a major factor in the rapid growth of the Indian entertainment industry. While watching a film, the viewers undergo an experience that evolves over time, thereby grabbing their attention. This triggers a sequence of processes which is perceptual, cognitive and emotional. Neurocinematics is an emerging field of research, that measures the cognitive responses of a film viewer. Neurocinematic studies, till date, have been performed using functional magnetic resonance imaging (fMRI); however recent studies have suggested the use of advancements in electroencephalography (EEG) in neurocinematics to address the issues involved with fMRI. In this article the emotions corresponding to two different genres of Indian films are captured with the real-time brainwaves of viewers using EEG and analyzed using R language.</i></p>

<pre xml:space="preserve">
@inproceedings{krishna_icaicr17,
  author={G, Gautham Krishna and Krishna, G and 
  Bhalaji, N},
  title = {Electroencephalography Based Analysis 
  of Emotions Among Indian Film Viewers},
  booktitle = {International Conference on Advanced
  Informatics for Computing Research},
  pages = {145--155},
  year = {2017},
  organization = {Springer}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://www.sciencedirect.com/science/article/pii/S1877050916305002" target="_blank"><img src="images/ICRTCSE16.png" alt="sym" width="80%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="http://www.sciencedirect.com/science/article/pii/S1877050916305002" id="RPL_ICRTCSE16" target="_blank">
      <heading>Analysis of Routing Protocol for Low-power & Lossy Networks in IoT Real Time Applications</heading>
      </a><br>
      <b><u>Gautham Krishna G</u></b>, G Krishna, N Bhalaji<br>
      <b>ICRTCSE 2016</b>, Procedia Computer Science
      </p>

      <div class="paper" id="rpl_icrtcse16">
      <a href="publications/RPLIoT_ICRTCSE16.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('rpl_icrtcse16_abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('rpl_icrtcse16')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="rpl_icrtcse16_abs">The wide-scaled sensing by Wireless Sensor Networks (WSN) has impacted several areas in the modern generation. It has offered the ability to measure, observe and understand the various physical factors from our environment. The rapid increase of WSN devices in an actuating-communicating network has led to the evolution of Internet of Things (IoT), where information is shared seamlessly across platforms by blending the sensors and actuators with our environment. These low cost WSN devices provide automation in medical and environmental monitoring. Evaluating the performance of these sensors using RPL enhances their use in real world applications. The realization of these RPL performances from different nodes focuses our study to utilize WSNs in our day-to-day applications. The effective sensor nodes (motes) for the appropriate environmental scenarios are analyzed, and we propose a collective view of the metrics for the same, for enhanced throughput in the given field of usage.</i></p>

<pre xml:space="preserve">
@article{krishna_icrtcse16,
  author = {Krishna, G Gautham and Krishna, G 
  and Bhalaji, N},
  title = {Analysis of Routing Protocol for Low-power
  and Lossy Networks in IoT Real Time Applications},
  journal = {Procedia Computer Science},
  volume = {87},
  pages = {270--274},
  year = {2016},
  publisher={Elsevier}
}
</pre>

      </div>
    </td>
  </tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><heading id="poster">&nbsp;&nbsp;Poster/Extended Abstract</heading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://mobiuk.org/" target="_blank"><img src="images/INTERSPEECH21.png" alt="sym" width="65%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://mobiuk.org/2021/programme.html" id="MOBIUK21" target="_blank">
      <heading>Heterogeneous Zero-Shot Federated Learning with New Classes for On-Device Audio Classification</heading></a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Satheesh Kumar Perepu<br>
      <b>MobiUK 2021</b> (Third UK Mobile, Wearable and Ubiquitous Systems Research Symposium)
      </p>

      <div class="paper" id="mobiuk21">
      <a href="https://mobiuk.org/2021/abstract/S1-P2_Gudur_HeterogeneousZeroShotFederatedLearning.pdf" target="_blank">extended abstract</a> /
      <a href="https://youtu.be/Nz-SDw0kfhc?t=1195" target="_blank">video</a> /
      <a href="slides/MobiUK21_Slides.pdf" target="_blank">slides</a>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://bayesiandeeplearning.org/" target="_blank"><img src="images/BDL20.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="http://bayesiandeeplearning.org/" id="ALHEALTH_BDL20" target="_blank">
      <heading>Bayesian Active Learning for Wearable and Mobile Health</heading></a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Abhijith Ragav, Prahalathan Sundaramoorthy, Venkatesh Umaashankar<br>
      <b>BDL 2020</b> (<b>NeurIPS</b> Europe meetup on Bayesian Deep Learning)
      </p>

      <div class="paper" id="alhealth_bdl20">
      <a href="publications/ALMobileHealth_BDL_NeurIPS20.pdf" target="_blank">poster</a>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://mobiuk.org/programme2019.html" target="_blank"><img src="images/EMDL19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://mobiuk.org/programme2019.html" id="MOBIUK19" target="_blank">
      <heading>Handling Real-time Unlabeled Data in Activity Recognition using Deep Bayesian Active Learning and Data Programming</heading></a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Prahalathan Sundaramoorthy, Venkatesh Umaashankar<br>
      <b>MobiUK 2019</b> (Second UK Mobile, Wearable and Ubiquitous Systems Research Symposium), University of Oxford
      </p>

      <div class="paper" id="mobiuk19">
      <a href="https://mobiuk.org/2019/abstract/S5-P4_Gudur_HandlingRealTimeUnlabeledData.pdf" target="_blank">extended abstract</a> /
      <a href="slides/MobiUK19_Slides.pdf" target="_blank">slides</a>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><img src="images/CBC15.png" alt="sym" width="60%" style="border-radius:15px"></td>
    <td width="67%" valign="top">
      <p><a href="publications/Neurocinematics_CBC15_Poster.pdf" id="NEUROCINEMATICS_CBC15" target="_blank">
      <heading>Neurocinematics: The Intelligent Review System</heading></a><br>
      N Bhalaji, G Krishna, <b><u>G Gautham Krishna</u></b><br>
      <b>CBC 2015</b> (3rd International Conference on Cognition, Brain and Computation), IIT Gandhinagar
      </p>

      <div class="paper" id="neurocinematics_cbc15">
      <a href="publications/Neurocinematics_CBC15_Poster.pdf" target="_blank">poster</a> /
      <a href="slides/CBC15_Slides.pdf" target="_blank">slides</a>

      </div>
    </td>
  </tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading id="patents">&nbsp;&nbsp;Patents</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td width="33%" valign="top"><img src="images/patents.jpeg	" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <ul>
        <li> <a href="https://patents.google.com/patent/WO2022013879A1/" id="FL_Heterogeneous_Patent" target="_blank">Federated Learning using Heterogeneous Labels</a></li>
        <li> <a href="https://patents.google.com/patent/WO2022162677A1/" id="FL_Newlabel_Patent" target="_blank">Distributed Machine Learning with New Labels using Heterogeneous Label Distribution</a></li>
        <li> <a href="https://patents.google.com/patent/WO2023166515A1/" id="AD_Patent" target="_blank">Method and Apparatus for Approach Recommendation with Threshold Optimization in Unsupervised Anomaly Detection</a></li>
      </ul>
      </p>
    </td>
  </tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading id="services">&nbsp;&nbsp;Services</sectionheading>
    <ul>
      <li> Program Committee Member/Reviewer</li>
      <ul>
        <li> ICASSP 2025</li>
        <li> ICML 2024 - Efficient Systems for Foundation Models Workshop (ES-FoMo)</li>
        <li> ICLR 2021 - Distributed and Private Machine Learning Workshop (DPML)</li> 
        <li> NeurIPS - Machine Learning for Health Workshop (ML4H)<br>
            - ML4H 2020, ML4H 2019
        </li>
        <li> KONVENS 2019, GermEval</li> 
      </ul>
      <li> Technical Reviewer of the book titled "Hands-On Meta Learning With Python"</li>
      <li> Event Organizer of "Data Nuggets" - a Data Science event, Invente 2016</li>
    </ul>
  </td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id="honorsandawards">&nbsp;&nbsp;Honors and Awards</sectionheading>
    <ul>
      <li><b>Graduate Ph.D. Fellowship</b> from Cockrell School of Engineering at UT Austin</li>
      <li> <a href="https://www.hackerrank.com/certificates/014169e8e7b2" target="_blank"><b>Top 1 percentile</b></a> in <a href="https://www.hackerrank.com/gauthamkrishna_g" target="_blank">HackerRank</a> – Algorithms Domain – Problem Solving (Advanced)</b></li>
      <li> Full financial registration grant to attend ICLR 2021, NeurIPS 2020 and OxML 2020</li>
      <li> Our project AIB (Automated Intelligent knowledge Base) won <b>Ericsson's Top Performance Competition 2020</b> in the Operational Excellence category</li>
      <li> Undergraduate financial research grant of <b>INR 25,000</b> from SSN College of Engineering</li>
      <li> <b>Winner</b> of GermEval Shared Task 1 Challenge - Subtask (a) @ KONVENS 2019 in post-evaluation phase</li>
      <li> Top 10 percentile in 42nd National Mathematics Talent Competitions, India</li>
      <li> Certification of Merit for Grade A1 in all subjects in AISSE (CBSE 10th boards)</li>
      <li> Completed all 10 levels of UCMAS Mental Arithmetic (Abacus)</li>
      <li> Division Level Badminton Player (U-19)</li>
      <li> 29th Rank overall in Grade 3 Keyboard</li>
    </ul>
  </td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id="talks">&nbsp;&nbsp;Talks</sectionheading>
    <ul>
      <li> Machine Learning and Ubiquitous Computing (June 2022)<br>
      SSN College of Engineering
      <li> Heterogeneous Zero-Shot Federated Learning with New Classes for On-Device Audio Classification (July 2021)<br>
      MobiUK 2021
      <li> Telecom-Specific Language Translation using GCP (May 2021)<br>
      Ericsson/Google Cloud Day
      <li> Resource-Constrained Machine Learning for Ubiquitous Computing Applications (Sept 2020)<br>
      <a href="images/flippedgaius20.jpeg" target="_blank">Flipped by GAIUS</a> /
      <a href="slides/FlippedGauis20_Slides.pdf" target="_blank">slides</a></li>      
    </ul>
  </td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id="summer_schools">&nbsp;&nbsp;Summer Schools</sectionheading>
    <ul>
      <li> <b><a href="summer_schools/SSAI2021_Certificate.pdf" target="_blank">5th Summer School on Artificial Intelligence</a></b> (Aug 2022)<br>
      Organizers – International Institute of Information Technology - Hyderabad (Remote – CVIT, IIIT-H)<br>
      Focus Areas – Computer Vision and Machine Learning
      <li> <b><a href="summer_schools/EEML2021_Certificate.pdf" target="_blank">Eastern European Machine Learning Summer School (EEML 2021)</a></b> (July 2021)<br>
      Organizers – DeepMind and others (Virtual – Budapest, Hungary)<br>
      Presented my work on <a href="https://www.youtube.com/watch?v=U_aWp6wajzM" target="_blank">Zero-shot Federated Learning with New Classes</a> as a poster.<br> Presented our idea on task-independent continual learning at the unconference sessions.
      <li> <b><a href="summer_schools/OxML2020_Certificate.pdf" target="_blank">Oxford Machine Learning Summer School (OxML 2020)</a></b> (Aug 2020)<br>
      Organizers – University of Oxford, AI for Global Goals, CIFAR, Saïd Business School, Deep Medicine<br>
      Attended with <b>full-fee waiver</b>.<br>
      Focus Areas – Deep Learning and Healthcare
      <li> <b><a href="summer_schools/EEML2020_Certificate.pdf" target="_blank">Eastern European Machine Learning Summer School (EEML 2020)</a></b> (July 2020)<br>
      Organizers – DeepMind and others (Virtual – Krakow, Poland)<br>
      Presented my work on <a href="https://www.youtube.com/watch?v=Kfy0URcPxyE&t" target="_blank">ActiveHARNet</a> as a poster.
    </ul>
  </td></tr>
</table>

    
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id="certifications">&nbsp;&nbsp;MOOCs/Certifications</sectionheading>
    <ul>
      <li> <a href="https://www.hackerrank.com/gauthamkrishna_g" target="_blank">HackerRank</a></li>
      <ul>
        <li> Problem Solving – <a href="https://www.hackerrank.com/certificates/014169e8e7b2" target="_blank">Advanced</a>, <a href="https://www.hackerrank.com/certificates/f99df227514e" target="_blank">Intermediate</a>, <a href="https://www.hackerrank.com/certificates/911b79a57138" target="_blank">Basic</a></li>
      </ul>
      <li> University of Washington, Coursera</li>
      &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.coursera.org/account/accomplishments/specialization/certificate/DSAS23JZKGG6" target="_blank">Machine Learning Specialization (4 courses)</a>
      <ul>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/KBN26TGYRFS2" target="_blank">Machine Learning Foundations: A Case Study Approach</a></li>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/6MD63T837DRW" target="_blank">Machine Learning: Regression</a></li>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/6WG9X8M3XDCD" target="_blank">Machine Learning: Classification</a></li>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/MM93FX3USFY3" target="_blank">Machine Learning: Clustering & Retrieval</a></li>
      </ul>
      <li> National Research University Higher School of Economics, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/" target="_blank">Bayesian Methods for Machine Learning</a></li>
      </ul>
      <li> Stanford University, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/JCU22MWZSVHS" target="_blank">Machine Learning</a></li>
      </ul>
      <li> University of California San Diego, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/DEUBHDVKGD3A" target="_blank">Algorithmic Toolbox</a></li>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/NULMGEKZ5CSY" target="_blank">Data Structures</a></li>
      </ul>
      <li> John Hopkins University, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/Y69XRGC2M35V" target="_blank">R Programming</a></li>
      </ul>
      <li> Stanford University</li>
      <ul>
        <li> <a href="http://cs231n.stanford.edu/" target="_blank">CS231n: Convolutional Neural Networks for Visual Recognition</a></li>
      </ul>
    </ul>
  </td></tr>
</table>

<!--Countries visited so far: India, UK, South Korea, Germany, Switzerland, France, Italy, Vatican, Luxembourg, Singapore, Malaysia, Thailand, SriLanka.-->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Template modified from <a href="http://www.cs.berkeley.edu/~barron/" target="_blank">this</a>, <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">this</a>, <a href="https://homes.cs.washington.edu/~kusupati/">this</a> and <a href="http://jeffdonahue.com/">this</a>.
    </font></p></td></tr>
</table>


</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('cladnet25_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('glumind25_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('attengluco25_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('svft24_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('enlsp24_neurips_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('hill22_calib_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icmla22_ad_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('interspeech21_newclassfl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('mlmh20_stress_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('dlhar20_heterofdl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('aiot20_heterofdl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icdmw19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('purba19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('germeval19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('emdl19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('emdl18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ficc18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('eeg_icaicr17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rpl_icrtcse16_abs');
</script>

</body>

</html>
