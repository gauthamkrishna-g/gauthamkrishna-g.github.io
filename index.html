<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110623657-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-110623657-1');
  </script>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron, Deepak Pathak, Aditya Kusupati and Jeff Donahue*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder {
    border-width: 1px;
    border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/favicon.ico" type="image/vnd.microsoft.icon">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Gautham Krishna Gudur</title>
  <meta name="Gautham Krishna Gudur's Homepage" http-equiv="Content-Type" content="Gautham Krishna Gudur's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <script src="js/scramble.js"></script>
</head>


<body>

<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <!-- <font size="7">Gautham Krishna Gudur</font><br> -->
    <pageheading>Gautham Krishna Gudur</pageheading><br>
    <b>email:</b> gauthamkrishna [at] utexas [dot] edu<br>
    &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; gauthamkrishna [dot] gudur [at] gmail [dot] com
    <!--<font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'dithgaag.kiamlrmo@auursguh.cmn', 
        [18,25,4,12,16,6,24,1,27,8,10,14,7,26,20,23,29,21,2,17,19,9,11,22,3,5,15,28,30,13]);
    </script>-->
  </p>

  <tr>
    <td width="32%" valign="top"><img src="images/avatar-icon.png" width="100%" style="border-radius:15px">
    <p align="center">
    <a href="CV_Gautham.pdf" target="_blank">CV</a> /
    <a href="https://scholar.google.co.in/citations?user=X5ThCEAAAAAJ" target="_blank">Google Scholar</a> <br/>
    <a href="https://www.linkedin.com/in/gauthamkrishna-g/" target="_blank">LinkedIn</a> /
    <a href="https://twitter.com/gauthamkrishna_" target="_blank">Twitter</a> <br/>
    <a href="https://github.com/gauthamkrishna-g" target="_blank">Github</a> /
    <a href="https://www.hackerrank.com/gauthamkrishna_g" target="_blank">HackerRank</a>
    </p>
    <p align="center"></p>
    </td>
    <td width="68%" valign="top" align="justify">
    <p>I am a Ph.D. student at <a href="https://www.utexas.edu/" target="_blank">The University of Texas at Austin</a> in the <a href="https://www.ece.utexas.edu/" target="_blank">Department of Electrical and Computer Engineering (ECE)</a>, advised by the wonderful <a href="https://users.ece.utexas.edu/~ethomaz/" target="_blank">Prof. Edison Thomaz</a>. I'm also a part of the <a href="https://wncg.org/" target="_blank">WNCG group</a>, primarily working on <i> resource-efficient</i>, <i>data-centric</i>, and <i>human-centric AI</i>. Previously, I worked as a Data Scientist at <a href="https://www.ericsson.com/" target="_blank">Ericsson R&D</a> in the Global AI Accelerator (GAIA) team in the space of machine intelligence and telecom. I've also worked at <a href="https://www.smartcardia.com/" target="_blank">SmartCardia</a> - an AI-assisted wearable healthcare spin-off from <a href="https://www.epfl.ch/en/" target="_blank">EPFL</a>.</p>
    <!--<font size=1>(Hit me up for interesting collaborations!)</font>-->
    <p>In a past life, I was a Research Assistant at <a href="https://www.solarillionfoundation.org/" target="_blank">Solarillion Foundation</a> working in areas of on-device ML. I earned a Bachelor's in Information Technology from <a href="https://www.ssn.edu.in/" target="_blank">SSN College of Engineering</a>, Chennai, where I also did some basic research on ML, IoT and HCI. In my time away from research, I travel, explore Indian classical/world music, play the badminton and DotA.</p>

<!--
<p align=center>
    <a href="Resume_Gautham.pdf" target="_blank">CV</a> /
    <a href="mailto:gauthamkrishna.gudur@gmail.com">Email</a> /
    <a href="bio.txt" target="_blank">Bio</a> /
    <a href="https://www.linkedin.com/in/gauthamkrishna-g/" target="_blank">LinkedIn</a> /
    <a href="https://scholar.google.co.in/citations?user=X5ThCEAAAAAJ" target="_blank">Google Scholar</a> /
    <a href="https://github.com/gauthamkrishna-g" target="_blank">Github</a> /
    <a href="https://twitter.com/gauthamkrishna_" target="_blank">Twitter</a> /
    <a href="https://www.hackerrank.com/gauthamkrishna_g" target="_blank">HackerRank</a>
</p>
-->
    </td>
  </tr>
</table>


<!--<p align="center">-->
<center>
[ <a href="#research_interests">Research Interests</a> | <a href="#news">News</a> | <a href="#publications">Publications</a> | <a href="#patents">Patents</a> | <a href="#services">Services</a> | <a href="#honorsandawards">Honors/Awards</a> | <a href="#talks">Talks</a> | <a href="#summer_schools">Summer Schools</a> | <a href="#certifications">MOOCs</a> ]
</center><br>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id='research_interests'>&nbsp;&nbsp;Research Interests</sectionheading><br>
     <!--Interdisciplinary with a good mix of theory/applied ML across multiple data regimes and domains.<br>-->
     <!--&nbsp;&nbsp;&nbsp;I'm broadly interested in the intersection of resource-efficient/human-centric learning and Generative AI, with a particular focus on:<br>-->
     <ul>
        <li> Accelerating training and inference using data-centric approaches by prioritizing important samples, in continual (curriculum/few-shot) learning and data-efficient human-in-the-loop settings
        <li> Ubiquitous computing, cross-modal learning, and on-device human-centric AI (wearable/audio sensing, human activity recognition, mobile health, etc.) with a focus on real-world deployability
        <li> Leveraging data/sample- and parameter-efficient techniques for foundation models and LLMs (Generative AI)</li>
        <li> Federated learning under statistical heterogeneities with new labels and models
        <li> Learning from limited supervision, particularly under data/label scarcity, sparsity, and calibrated uncertainty
        <!--<li> Leveraging explainable components for guided neural network training-->
      </ul>
      Broadly, <b>resource-efficient</b> and <b>resource-aware learning</b>,<br>
      where <b>resource := data, sample, label, model, parameter, compute, etc.</b><br>

      <p>Here are a <a href="javascript:toggleblock('research_keywords')">few keywords</a> that might best describe my research interests <font size=1>(present/past, hopefully in the future!)</font>.
        <div id="research_keywords" style="display:none">
          <b>
            <ul>
              <li>Machine (Deep, Bayesian, Active, Continual, Federated, Curriculum, Few-shot, Human-in-the-Loop) Learning</li>
              <li>Generative AI, LLMs, Parameter-Efficient Fine-Tuning (PEFT), Foundation Models</li>
              <li>Ubiquitous Computing (Mobile/Wearable Sensing, Human Activity Recognition, Audio/Cross-modal Sensing) and On-Device Training</li>
              <li>Limited Supervision, Subset/Model Selection, Uncertainty-Aware Learning, Sparsity</li>
            </ul>
          </b>
        </div></p>
      <!--I've published my works at conferences and workshop venues like ACM UbiComp/ISWC, NeurIPS, Interspeech, ICML, ICLR, KDD, MobiSys-->
    </td></tr>
</table>


<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr>
    <td width="33%" valign="top" align="center"><a href="publications/wordcloud_research.png" target="_blank"><img src="publications/wordcloud_research.png" alt="sym" width="100%" style="border-radius:15px;"></a><br>
      This is what <a href="http://wordle.net" target="_blank">Wordle</a> thinks of my publication titles.
    </td>
    <td width="67%" valign="top">
      <p>
      Broadly, <b>resource-efficient</b> and <b>resource-aware learning</b>,<br>
      where <b>resource := data, sample, label, model, parameter, compute, etc.</b><br>
      </p>
      <p><a href="javascript:toggleblock('research_keywords')">Few Keywords</a> that might best describe my research interests<font size=1><br>(present/past, hopefully in the future!)</font>.
        <div id="research_keywords" style="display:none">
          <b>
            <ul>
              <li>Machine (Deep, Bayesian, Active, Continual, Federated, Curriculum, Few-shot, Human-in-the-Loop) Learning</li>
              <li>Generative AI, LLMs, Foundation Models</li>
              <li>Limited Supervision, Subset/Model Selection</li>
              <li>Ubiquitous Computing (Mobile/Wearable Sensing, Human Activity Recognition, Audio/Cross-modal Sensing) and On-Device Training</li>
            </ul>
          </b>
        </div></p>
    </td>
  </tr>
</table>-->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">

  <tr><td>
    <sectionheading id='news'>&nbsp;&nbsp;What's New!</sectionheading>
    <ul>
      <li> <a href="#SVFT24">SVFT</a> accepted at the WANT (<i>oral presentation</i>) and ES-FoMo workshops @ ICML 2024!</li>
      <li> New work on <a href="#SVFT24">SVFT</a>: a parameter-efficient fine-tuning (PEFT) technique with singular vectors, is now on arXiv.</li>
      <li> I was awarded the Graduate Ph.D. Fellowship from Cockrell School of Engineering at UT Austin!</li>
      <li> I'll be joining the <a href="https://www.utexas.edu/" target="_blank"> University of Texas at Austin </a> in August 2023 for a Ph.D. in the <a href="https://ece.utexas.edu/" target="_blank">Chandra Department of ECE</a>, primarily working at the intersection of resource-efficient and human-centric ML. Go Longhorns! <img src="images/texas_longhorns.png" alt="longhorns" width="4%"></li>
		<li> <a href="#HILL22_CALIB">Can Calibration Improve Sample Prioritization?</a> accepted at the NeurIPS 2022 Workshop on Human in the Loop Learning (HILL) and the Has It Trained Yet? (HITY) workshops.</li>
		<!--<li> <a href="#IMWUT22_FL">Federated Learning for Continuous Heterogeneous Sensing</a> to be submitted soon.</li>-->
		<li> <a href="#ICMLA22_AD">Model Selection in Unsupervised Anomaly Detection</a> accepted at IEEE ICMLA 2022 for <i>oral presentation</i>. A <a href="#AD_Patent">patent</a> was also filed on the same with automatic threshold optimization.</li>
      <li> Attended the amazing <a href="https://www.eeml.eu/" target="_blank">EEML 2021</a> again! Also, presented my poster/video on <a href="#INTERSPEECH21">Zero-Shot FL with New Classes</a>.</li>
      <li> Presented a talk on <a href="#MOBIUK21">On-Device Zero-Shot FL with New Classes</a> at MobiUK 2021.</li>
      <li> <a href="#INTERSPEECH21">Zero-Shot Federated Learning with New Classes for Audio Classification</a> accepted at INTERSPEECH 2021. Abridged versions accepted at DPML and HAET workshops @ ICLR 2021.</li><!--<a href="#MOBIUK21">MobiUK 2021</a> and EEML 2021.-->
      <li> First time at NeurIPS (with full registration grant)!<br>
        - Two papers – <a href="#MLMH20_STRESS">Bayesian Active Learning for Wearable Stress Detection</a> and <a href="#DLHAR20_HETEROFDL">Federated Learning with Heterogeneous Labels and Models for HAR</a> accepted at ML for Mobile Health Workshop @ NeurIPS 2020.<br>
        - <a href="#ALHEALTH_BDL20">Bayesian Active Learning for Wearable and Mobile Health</a> poster also accepted at NeurIPS Europe meetup on Bayesian Deep Learning 2020.</li>

    <a href="javascript:toggleblock('oldnews')">-- more --</a>
    <div id="oldnews" style="display:none">
      <li> <a href="#patents">Two new patents</a> filed on handling heterogeneous and new labels during federated learning.
      <li> Our project AIB (Automated Intelligent knowledge Base) won Ericsson's Top Performance Competition 2020.</li>
      <li> <a href="https://www.ericsson.com/en/blog/2020/7/how-to-make-anomaly-detection-more-accessible" target="_blank">E-ADF featured on Ericsson blog</a>. E-ADF is our in-house end-to-end framework for anomaly detection.
      <li> <a href="#DLHAR20_HETEROFDL">Resource-Constrained Federated Learning with Heterogeneous Labels and Models for HAR</a> accepted at <br>DL-HAR Workshop @ IJCAI 2020. An <a href="#AIoT20_HETEROFDL">abridged version</a> (with vision) accepted at AIoT Workshop @ KDD 2020.</li>
      <li> Gave a talk on <a href="#talks">Resource-Constrained Machine Learning for Ubiquitous Computing</a> at Flipped by GAIUS.</li>
      <li> Had a great time attending the amazing <a href="https://www.eeml.eu/" target="_blank">EEML 2020</a> and <a href="https://www.oxfordml.school/" target="_blank">OxML 2020</a>! Presented my poster/video on <a href="#EMDL19">ActiveHARNet</a> at EEML, and obtained a full-fee waiver to attend OxML.</li>
      <li> <a href="#ICDMW19">A Dynamic Adaptive Movie Occupancy Forecasting System</a> accepted at LMID Workshop @ IEEE ICDM 2019.</li>
      <li> We won the Shared Task 1 Challenge, Subtask (a) in post-evaluation phase at GermEval @ KONVENS 2019. <a href="#GERMEVAL19">Label Frequency Transformation for Multi-Label Multi-Class Text Classification</a> has been accepted here.</li>
      <li> <a href="#PURBA19">On-Device Intelligent Bus Stop Recognition System</a> accepted at PURBA Workshop @ ACM UbiComp 2019.</li>
      <li> <a href="#EMDL19">ActiveHARNet</a> accepted at Embedded and Mobile Deep Learning Workshop @ ACM MobiSys 2019.</li>
      <li> <a href="#MOBIUK19">Handling Real-time Unlabeled Data in HAR</a> presented at MobiUK 2019, University of Oxford.</li>
      <li> Joined Global AI Accelerator (GAIA) team at Ericsson R&D, broadly working on solving ML problems in telecom.</li>
      <li> <a href="#EMDL18">HARNet</a> accepted at Embedded and Mobile Deep Learning Workshop @ ACM MobiSys 2018.</li>
      <li> Joined SmartCardia (EPFL) as a Machine Learning Engineer, working on AI assisted wearable health-care.</li>
      <li> <a href="#FICC18">Multi-modal Dynamic Gesture Recognition System using Machine Learning</a> accepted at IEEE FICC 2018.</li>
    </div>
    </ul>
  </td></tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr>
    <td>
    <sectionheading id="publications">&nbsp;&nbsp;&nbsp;&nbsp;Publications</sectionheading> &nbsp;&nbsp;&nbsp;[ <a href="#preprints" align="center">Preprints</a> | <a href="#conference" align="center">Conference/Journal/Workshop</a> | <a href="#poster">Poster/Extended Abstract</a> ]<br>
    <center>* indicates joint first authors</center>
    </td>
  </tr>
</table>

<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr>
    <td>
    <sectionheading id="publications">&nbsp;&nbsp;Publications</sectionheading><br>
&nbsp;&nbsp;&nbsp;[ <a href="#under_progress" align="center">To be Submitted/Research Under Progress</a> | <a href="#conference" align="center">Conference/Journal/Workshop</a> | <a href="#poster">Poster/Extended Abstract</a> ]<br>
    <center><font size=1>* Equal Contribution</font></center>
    </td>
  </tr>
</table>-->

<!--
<center>
[ <a href="#conference">Conference/Journal/Workshop</a> | <a href="#poster">Poster/Extended Abstracts</a>]
<center><font size=1>* Equal Contribution</font></center>
</center>
<br>
-->

<!--
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><heading id="under_progress">&nbsp;&nbsp;To be Submitted/Research Under Progress</heading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top" align="center"></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Can Explanations Improve Curriculum Learning and Subset Selection?</heading><br>
      <i>Research Under Progress</i><br>
      </p>

      <div class="paper" id="imwut22_fl">
      <a href="javascript:toggleblock('icmla22_ad_abs')">abstract</a>

      <p align="justify"> <i id="icmla22_ad_abs">Anomaly Detection is a widely used technique in machine learning that identifies context-specific outliers. Most real-world anomaly detection applications are unsupervised, owing to the complexity of obtaining labeled data for the given context. In this paper, we solve two important problems pertaining to unsupervised anomaly detection. First, we identify only the most informative subsets of data points to obtain the ground truths from the domain expert (oracle); second, we perform efficient model selection using a Bayesian Inference framework and recommend the top-k models to be fine-tuned prior to deployment. To this end, we exploit multiple existing and novel acquisition functions, and successfully demonstrate the effectiveness of the proposed framework using a weighted Ranking Score (\eta) to accurately rank the top-k models. Our empirical results show a significant reduction in data points acquired (with at least 60% reduction) while not compromising on the efficiency of the top-k models chosen.</i></p>

      </div>
    </td>
  </tr>

</table>-->

<!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><heading id="preprints">&nbsp;&nbsp;Preprints</heading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

</table>-->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><heading id="conference">&nbsp;&nbsp;Conference/Journal/Workshop</heading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2405.19597" target="_blank"><img src="images/SVFT24.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <a href="https://arxiv.org/abs/2405.19597" id="SVFT24" target="_blank">
      <heading>SVFT: Parameter-Efficient Fine-Tuning with Singular Vectors</heading>
      </a><br>
      Vijay Lingam*, Atula Tejaswi*, Aditya Vavre*, Aneesh Shetty*, <b><u>Gautham Krishna Gudur</u></b>*, Joydeep Ghosh, Alex Dimakis, Eunsol Choi, Aleksandar Bojchevski, Sujay Sanghavi<br>
      Workshop on Advancing Neural Network Training (WANT): Computational Efficiency, Scalability, and Resource Optimization <i>[Oral Presentation]</i><br>
      Workshop on Efficient Systems for Foundation Models (ES-FoMo)<br>
      <b>ICML 2024</b>
      </p>
  
      <div class="paper" id="svft24">
      <a href="https://openreview.net/pdf?id=DOUskwCqg5" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('svft24_abs')">abstract</a> /
      <a href="https://github.com/VijayLingam95/SVFT/" target="_blank">code</a> /
      <a href="https://x.com/atu_tej/status/1799519888588837273" target="_blank">tweet</a> /
      <a shape="rect" href="javascript:togglebib('svft24')" class="togglebib">bibtex</a>
  
      <p align="justify"> <i id="svft24_abs">Popular parameter-efficient fine-tuning (PEFT) methods, such as LoRA and its variants, freeze pre-trained model weights <b>W</b> and inject learnable matrices <b>∆W</b>. These <b>∆W</b> matrices are structured for efficient parameterization, often using techniques like low-rank approximations or scaling vectors. However, these methods typically show a performance gap compared to full fine-tuning. Although recent PEFT methods have narrowed this gap, they do so at the cost of additional learnable parameters. We propose SVFT, a simple approach that fundamentally differs from existing methods: the structure imposed on <b>∆W</b> depends on the specific weight matrix <b>W</b>. Specifically, SVFT updates <b>W</b> as a sparse combination of outer products of its singular vectors, training only the coefficients (scales) of these sparse combinations. This approach allows fine-grained control over expressivity through the number of coefficients. Extensive experiments on language and vision benchmarks show that SVFT recovers up to 96% of full fine-tuning performance while training only 0.006 to 0.25% of parameters, outperforming existing methods that only recover up to 85% performance using 0.03 to 0.8% of the trainable parameter budget.</i></p>
  
  <pre xml:space="preserve">
  @inproceedings{lingam_svft24,
  title={SVFT: Parameter-Efficient Fine-Tuning with
  Singular Vectors},
  author={Lingam, Vijay and Tejaswi, Atula
  and Vavre, Aditya and Shetty, Aneesh
  and Gudur, Gautham Krishna and Ghosh, Joydeep
  and Dimakis, Alex and Choi, Eunsol and
  Bojchevski, Aleksandar and Sanghavi, Sujay},
  booktitle={2nd Workshop on Advancing Neural
  Network Training: Computational Efficiency,
  Scalability, and Resource Optimization
  (WANT@ICML 2024)},
  year={2024},
  url={https://openreview.net/forum?id=DOUskwCqg5}
  }
  </pre>
  
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2210.06592" target="_blank"><img src="images/HILL22.png" alt="sym" width="100%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <a href="https://arxiv.org/abs/2210.06592" id="HILL22_CALIB" target="_blank">
      <heading>Can Calibration Improve Sample Prioritization?</heading>
      </a><br>
      Ganesh Tata*, <b><u>Gautham Krishna Gudur</u></b>*, Gopinath Chennupati, Mohammad Emtiyaz Khan<br>
      Human in the Loop Learning (HILL) Workshop<br>
      Has It Trained Yet? (HITY) Workshop<br><b>NeurIPS 2022</b>
      </p>

      <div class="paper" id="hill22_calib">
      <a href="https://openreview.net/pdf?id=LnygZu8WJk" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('hill22_calib_abs')">abstract</a> /
      <a href="publications/Calib_NeurIPS22_Poster.pdf" target="_blank">poster</a> / 
      <a href="https://github.com/tataganesh/Calib-Sample-Prioritization" target="_blank">code</a> /
      <a shape="rect" href="javascript:togglebib('hill22_calib')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="hill22_calib_abs">Calibration can reduce overconfident predictions of deep neural networks, but can calibration also accelerate training? In this paper, we show that it can when used to prioritize some examples for performing subset selection. We study the effect of popular calibration techniques in selecting better subsets of samples during training (also called sample prioritization) and observe that calibration can improve the quality of subsets, reduce the number of examples per epoch (by at least 70%), and can thereby speed up the overall training process. We further study the effect of using calibrated pre-trained models coupled with calibration during training to guide sample prioritization, which again seems to improve the quality of samples selected.</i></p>

<pre xml:space="preserve">
@inproceedings{tata_hity22,
  author = {Tata, Ganesh and Gudur, Gautham Krishna
  and Chennupati, Gopinath and 
  Khan, Mohammad Emtiyaz},
  title = {Can Calibration Improve Sample 
  Prioritization?},
  booktitle = {Has it Trained Yet? 
  NeurIPS 2022 Workshop},
  year = {2022}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://ieeexplore.ieee.org/document/10069418" target="_blank"><img src="images/ICMLA22.png" alt="sym" width="100%" style="border-radius:15px;"></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <a href="https://ieeexplore.ieee.org/document/10069418" id="ICMLA22_AD" target="_blank">
      <heading>Data-Efficient Automatic Model Selection in Unsupervised Anomaly Detection</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Raaghul R, Adithya K, Shrihari Vasudevan<br>
      <b>IEEE ICMLA 2022</b> <i>[Oral Presentation]</i><br>
      </p>

      <div class="paper" id="icmla22_ad">
      <a href="publications/BayesianAD_ICMLA22.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('icmla22_ad_abs')">abstract</a> /
      <a href="slides/ICMLA22_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('icmla22_ad')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="icmla22_ad_abs">Anomaly Detection is a widely used technique in machine learning that identifies context-specific outliers. Most real-world anomaly detection applications are unsupervised, owing to the bottleneck of obtaining labeled data for a given context. In this paper, we solve two important problems pertaining to unsupervised anomaly detection. First, we identify only the most informative subsets of data points and obtain ground truths from the domain expert (oracle); second, we perform efficient model selection using a Bayesian Inference framework and recommend the top-k models to be fine-tuned prior to deployment. To this end, we exploit multiple existing and novel acquisition functions, and successfully demonstrate the effectiveness of the proposed framework using a weighted Ranking Score (\eta) to accurately rank the top-k models. Our empirical results show a significant reduction in data points acquired (with at least 60% reduction) while not compromising on the efficiency of the top-k models chosen, with both uniform and non-uniform priors over models.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_icmla22,
  author = {Gudur, Gautham Krishna and Raaghul, R
  and Adithya, K and Vasudevan, Shrihari},
  title = {Data-Efficient Automatic Model 
  Selection in Unsupervised Anomaly Detection},
  booktitle = {IEEE ICMLA 2022},
  year = {2022}
}
</pre>

      </div>
    </td>
  </tr>
  
  <tr>
    <td width="33%" valign="top" align="center"><a href="https://www.isca-speech.org/archive/interspeech_2021/gudur21_interspeech.html" target="_blank"><img src="images/INTERSPEECH21.png" alt="sym" width="65%" style="border-radius:15px;"></a></td>
    <td width="67%" valign="top" style="padding-top:15px">
      <p>
      <a href="https://www.isca-archive.org/interspeech_2021/gudur21_interspeech.html" id="INTERSPEECH21" target="_blank">
      <heading>Zero-Shot Federated Learning with New Classes for Audio Classification</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Satheesh Kumar Perepu<br>
      <b>INTERSPEECH 2021</b><br>
      <a href="https://dp-ml.github.io/2021-workshop-ICLR/files/44.pdf" target="_blank">Abridged version</a>: Distributed and Private Machine Learning (DPML) & Hardware Aware Efficient Training (HAET) workshops, <b>ICLR 2021</b><br>
      Also presented as a <a href="https://www.youtube.com/watch?v=U_aWp6wajzM" target="_blank">poster</a> at <b>EEML 2021</b>
      </p>

      <div class="paper" id="interspeech21_newclassfl">
      <a href="https://www.isca-archive.org/interspeech_2021/gudur21_interspeech.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('interspeech21_newclassfl_abs')">abstract</a> /
      <a href="publications/NewClassFL_ICLR21_Poster.pdf" target="_blank">poster</a> /
      <a href="https://www.youtube.com/watch?v=U_aWp6wajzM" target="_blank">video</a> /
      <a href="slides/DPML21_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('interspeech21_newclassfl')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="interspeech21_newclassfl_abs">Federated learning is an effective way of extracting insights from different user devices while preserving the privacy of users. However, new classes with completely unseen data distributions can stream across any device in a federated learning setting, whose data cannot be accessed by the global server or other users. To this end, we propose a unified zero-shot framework to handle these aforementioned challenges during federated learning. We simulate two scenarios here – 1) when the new class labels are not reported by the user, the traditional FL setting is used; 2) when new class labels are reported by the user, we synthesize Anonymized Data Impressions by calculating class similarity matrices corresponding to each device’s new classes followed by unsupervised clustering to distinguish between new classes across different users. Moreover, our proposed framework can also handle statistical heterogeneities in both labels and models across the participating users. We empirically evaluate our framework on-device across different communication rounds (FL iterations) with new classes in both local and global updates, along with heterogeneous labels and models, on two widely used audio classification applications – keyword spotting and urban sound classification, and observe an average deterministic accuracy increase of ∼4.041% and ∼4.258% respectively.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_interspeech21,
  author = {Gudur, Gautham Krishna and 
  Perepu, Satheesh Kumar},
  title = {Zero-Shot Federated Learning with 
  New Classes for Audio Classification},
  booktitle = {Proc. Interspeech 2021},
  pages = {1579--1583},
  year = {2021},
  doi = {10.21437/Interspeech.2021-2264}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2012.02702" target="_blank"><img src="images/MLMH20_Stress.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://arxiv.org/abs/2012.02702" id="MLMH20_STRESS" target="_blank">
      <heading>Bayesian Active Learning for Wearable Stress and Affect Detection</heading>
      </a><br>
      Abhijith Ragav*, <b><u>Gautham Krishna Gudur</u></b>*<br>
      Machine Learning for Mobile Health Workshop<br><b>NeurIPS 2020</b>
      </p>

      <div class="paper" id="mlmh20_stress">
      <a href="https://arxiv.org/pdf/2012.02702.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('mlmh20_stress_abs')">abstract</a> /
      <a href="publications/ALStress_MLMH_NeurIPS20_Poster.pdf" target="_blank">poster</a> /
      <a shape="rect" href="javascript:togglebib('mlmh20_stress')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="mlmh20_stress_abs">In the recent past, psychological stress has been increasingly observed in humans, and early detection is crucial to prevent health risks. Stress detection using ondevice deep learning algorithms has been on the rise owing to advancements in pervasive computing. However, an important challenge that needs to be addressed is handling unlabeled data in real-time via suitable ground truthing techniques (like Active Learning), which should help establish affective states (labels) while also selecting only the most informative data points to query from an oracle. In this paper, we propose a framework with capabilities to represent model uncertainties through approximations in Bayesian Neural Networks using Monte-Carlo (MC) Dropout. This is combined with suitable acquisition functions for active learning. Empirical results on a popular stress and affect detection dataset experimented on a Raspberry Pi 2 indicate that our proposed framework achieves a considerable efficiency boost during inference, with a substantially low number of acquired pool points during active learning across various acquisition functions. Variation Ratios achieves an accuracy of 90.38% which is comparable to the maximum test accuracy achieved while training on about 40% lesser data.</i></p>

<pre xml:space="preserve">
@article{ragav_mlmh20,
  author = {Ragav, Abhijith and 
  Gudur, Gautham Krishna},
  title = {Bayesian Active Learning for 
  Wearable Stress and Affect Detection},
  journal = {arXiv preprint arXiv:2012.02702},
  year = {2020}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://link.springer.com/chapter/10.1007/978-981-16-0575-8_5" target="_blank"><img src="images/DLHAR20.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://link.springer.com/chapter/10.1007/978-981-16-0575-8_5" id="DLHAR20_HETEROFDL" target="_blank">
      <heading>Resource-Constrained Federated Learning with Heterogeneous Labels and Models for Human Activity Recognition</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Satheesh Kumar Perepu<br>
      2nd International Workshop on Deep Learning for Human Activity Recognition (DL-HAR), <b>IJCAI-PRICAI 2020</b><br>
      <a href="https://arxiv.org/pdf/2012.02539.pdf" target="_blank">Abridged version</a>: Machine Learning for Mobile Health Workshop,<br> <b>NeurIPS 2020</b>
      </p>

      <div class="paper" id="dlhar20_heterofdl">
      <a href="publications/HeteroFDL_DLHAR_IJCAI20.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('dlhar20_heterofdl_abs')">abstract</a> /
      <a href="publications/HeteroFDL_MLMH_NeurIPS20_Poster.pdf" target="_blank">poster</a> /
      <a href="slides/DLHAR20_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('dlhar20_heterofdl')" class="togglebib">bibtex</a>
      <br>
      <p align="justify"> <i id="dlhar20_heterofdl_abs">One of the most significant applications in pervasive computing for modeling user behavior is Human Activity Recognition (HAR). Such applications necessitate us to characterize insights from multiple resource-constrained user devices using machine learning techniques for effective personalized activity monitoring. On-device Federated Learning proves to be an extremely viable option for distributed and collaborative machine learning in such scenarios, and is an active area of research. However, there are a variety of challenges in addressing statistical (non-IID data) and model heterogeneities across users. In addition, in this paper, we explore a new challenge of interest - to handle heterogeneities in labels (activities) across users during federated learning. To this end, we propose a framework with two different versions for federated label-based aggregation, which leverage overlapping information gain across activities - one using Model Distillation Update, and the other using Weighted \alpha$-update. Empirical evaluation on the Heterogeneity Human Activity Recognition (HHAR) dataset (with four activities for effective elucidation of results) indicates an average deterministic accuracy increase of at least ~11.01% with the model distillation update strategy and ~9.16% with the weighted \alpha-update strategy. We demonstrate the on-device capabilities of our proposed framework by using Raspberry Pi 2, a single-board computing platform.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_dlhar20,
  author = {Gudur, Gautham Krishna and 
  Perepu, Satheesh K},
  title = {Resource-Constrained Federated Learning
  with Heterogeneous Labels and Models for 
  Human Activity Recognition},
  booktitle = {Deep Learning for Human Activity 
  Recognition},
  pages = {55--69},
  year = {2021},
  publisher={Springer Singapore}
}

@article{gudur_mlmh20,
  author = {Gudur, Gautham Krishna and
  Perepu, Satheesh K},
  title = {Federated Learning with Heterogeneous 
  Labels and Models for Mobile Activity 
  Monitoring},
  journal = {arXiv preprint arXiv:2012.02539},
  year = {2020}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://arxiv.org/abs/2011.03206" target="_blank"><img src="images/AIoT20.png" alt="sym" width="75%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://arxiv.org/abs/2011.03206" id="AIoT20_HETEROFDL" target="_blank">
      <heading>Resource-Constrained Federated Learning with Heterogeneous Labels and Models</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Bala Shyamala Balaji, Satheesh Kumar Perepu<br>
      3rd International Workshop on Artificial Intelligence of Things (AIoT)<br><b>ACM KDD 2020</b>
      </p>

      <div class="paper" id="aiot20_heterofdl">
      <a href="https://aiotworkshop.github.io/published/AIoT_10_Gudur_TechnicalPaper_KDD2020.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('aiot20_heterofdl_abs')">abstract</a> /
      <a href="slides/AIoT20_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('aiot20_heterofdl')" class="togglebib">bibtex</a>
      <br>
      <p align="justify"> <i id="aiot20_heterofdl_abs">Various IoT applications demand resource-constrained machine learning mechanisms for different applications such as pervasive healthcare, activity monitoring, speech recognition, real-time computer vision, etc. This necessitates us to leverage information from multiple devices with few communication overheads. Federated Learning proves to be an extremely viable option for distributed and collaborative machine learning. Particularly, on-device federated learning is an active area of research, however, there are a variety of challenges in addressing statistical (non-IID data) and model heterogeneities. In addition, in this paper we explore a new challenge of interest - to handle label heterogeneities in federated learning. To this end, we propose a framework with simple $\alpha$-weighted federated aggregation of scores which leverages overlapping information gain across labels, while saving bandwidth costs in the process. Empirical evaluation on Animals-10 dataset (with 4 labels for effective elucidation of results) indicates an average deterministic accuracy increase of at least ~16.7%. We also demonstrate the on-device capabilities of our proposed framework by experimenting with federated learning and inference across different iterations on a Raspberry Pi 2, a single-board computing platform.</i></p>

<pre xml:space="preserve">
@article{gudur_aiot20,
  author = {Gudur, Gautham Krishna and Balaji,
  Bala Shyamala and Perepu, Satheesh K},
  title = {Resource-Constrained Federated Learning
  with Heterogeneous Labels and Models},
  journal = {arXiv preprint arXiv:2011.03206},
  year = {2020}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://ieeexplore.ieee.org/document/8955583" target="_blank"><img src="images/ICDMW19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://ieeexplore.ieee.org/document/8955583" id="ICDMW19" target="_blank">
      <heading>A Dynamically Adaptive Movie Occupancy Forecasting System with Feature Optimization</heading>
      </a><br>
      Sundararaman Venkataramani, Ateendra Ramesh, Sharan Sundar S, Aashish Kumar Jain, <b><u>Gautham Krishna Gudur</u></b>, Vineeth Vijayaraghavan<br>
      Workshop on Learning and Mining with Industrial Data (LMID)<br><b>IEEE ICDM 2019</b>
      </p>

      <div class="paper" id="icdmw19">
      <a href="publications/LMID_ICDM19.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('icdmw19_abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('icdmw19')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="icdmw19_abs">Demand Forecasting is a primary revenue management strategy in any business model, particularly in the highly volatile entertainment/movie industry wherein, inaccurate forecasting may lead to loss in revenue, improper workforce allocation and food wastage or shortage. Predominant challenges in Occupancy Forecasting might involve complexities in modeling external factors – particularly in Indian multiplexes with multilingual movies, high degrees of uncertainty in crowdbehavior, seasonality drifts, influence of socio-economic events and weather conditions. In this paper, we investigate the problem of movie occupancy forecasting, a significant step in the decision making process of movie scheduling and resource management, by leveraging the historical transactions performed in a multiplex consisting of eight screens with an average footfall of over 5500 on holidays and over 3500 on nonholidays every day. To effectively capture crowd behavior and predict the occupancy, we engineer and benchmark behavioral features by structuring recent historical transaction data spanning over five years from one of the top Indian movie multiplex chains, and propose various deep learning and conventional machine learning models. We also propose and optimize on a novel feature called Sale Velocity to incorporate the dynamic crowd behavior in movies. The performance of these models are benchmarked in real-time using Mean Absolute Percentage Error (MAPE), and found to be highly promising while substantially outperforming a domain expert’s predictions.</i></p>

<pre xml:space="preserve">
@inproceedings{venkataramani_icdmw19,
  author = {Venkataramani, Sundararaman and 
  Ramesh, Ateendra and S, Sharan Sundar and 
  Jain, Aashish Kumar and Gudur, Gautham Krishna
  and Vijayaraghavan, Vineeth},
  title = {A Dynamically Adaptive Movie Occupancy 
  Forecasting System with Feature Optimization},
  booktitle = {International Conference on Data 
  Mining Workshops (ICDMW)},
  pages = {799--805},
  year = {2019},
  organization = {IEEE}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dl.acm.org/doi/10.1145/3341162.3349323" target="_blank"><img src="images/PURBA19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://dl.acm.org/doi/10.1145/3341162.3349323" id="PURBA19" target="_blank">
      <heading>A Vision-based Deep On-Device Intelligent Bus Stop Recognition System</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Ateendra Ramesh, Srinivasan R<br>
      8th International Workshop on Pervasive Urban Applications (PURBA)<br><b>ACM UbiComp 2019</b> <i>[Oral Presentation]</i>
      </p>

      <div class="paper" id="purba19">
      <a href="https://cpemis.eng.cmu.ac.th/~santi/purba2019/papers/p23.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('purba19_abs')">abstract</a> /
      <a href="https://github.com/gauthamkrishna-g/Intelligent-Bus-Stop-Recognition-System" target="_blank">code</a> /
      <a href="slides/PURBA19_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('purba19')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="purba19_abs">Intelligent public transportation systems are the cornerstone to any smart city, given the advancements made in the field of self-driving autonomous vehicles - particularly for autonomous buses, where it becomes really difficult to systematize a way to identify the arrival of a bus stop on-the-fly for the bus to appropriately halt and notify its passengers. This paper proposes an automatic and intelligent bus stop recognition system built on computer vision techniques, deployed on a low-cost single-board computing platform with minimal human supervision. The on-device recognition engine aims to extract the features of a bus stop and its surrounding environment, which eliminates the need for a conventional Global Positioning System (GPS) look-up, thereby alleviating network latency and accuracy issues. The dataset proposed in this paper consists of images of 11 different bus stops taken at different locations in Chennai, India during day and night. The core engine consists of a convolutional neural network (CNN) of size ~260 kB that is computationally lightweight for training and inference. In order to automatically scale and adapt to the dynamic landscape of bus stops over time, incremental learning (model updation) techniques were explored on-device from real-time incoming data points. Real-time incoming streams of images are unlabeled, hence suitable ground truthing strategies (like Active Learning), should help establish labels on-the-fly. Light-weight Bayesian Active Learning strategies using Bayesian Neural Networks using dropout (capable of representing model uncertainties) enable selection of the most informative images to query from an oracle. Intelligent rendering of the inference module by iteratively looking for better images on either sides of the bus stop environment propels the system towards human-like behavior. The proposed work can be integrated seamlessly into the widespread existing vision-based self-driving autonomous vehicles.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_purba19,
  author = {Gudur, Gautham Krishna and Ramesh, 
  Ateendra and R, Srinivasan},
  title = {A Vision-Based Deep On-Device 
  Intelligent Bus Stop Recognition System},
  booktitle = {Adjunct Proceedings of the 2019 ACM  
  International Joint Conference on Pervasive and
  Ubiquitous Computing and Proceedings of the 2019 
  ACM International Symposium on Wearable Computers},
  pages = {963--968},
  numpages = {6},
  year = {2019}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://competitions.codalab.org/competitions/20139" target="_blank"><img src="images/GermEval19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://competitions.codalab.org/competitions/20139" id="GERMEVAL19" target="_blank">
      <heading>Label Frequency Transformation for Multi-Label Multi-Class Text Classification</heading>
      </a><br>
      Raghavan A K, Venkatesh Umaashankar, <b><u>Gautham Krishna Gudur</u></b><br>
      GermEval, <b>KONVENS 2019</b><br>
      (<b>Winner</b> of Shared Task 1, Subtask (a) in post-evaluation phase)
      </p>

      <div class="paper" id="germeval19">
      <a href="https://konvens.org/proceedings/2019/papers/germeval/Germeval_Task1_paper_8.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('germeval19_abs')">abstract</a> /
      <a href="https://gauthamkrishna-g.github.io/publications/GermEval_KONVENS19_Poster.pdf" target="_blank">poster</a> /
      <a href="https://github.com/oneraghavan/germeval-2019" target="_blank">code</a> /
      <a shape="rect" href="javascript:togglebib('germeval19')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="germeval19_abs">In this paper, we (Team Raghavan) describe the system of our submission for GermEval 2019 Task 1 - Subtask (a) and Subtask (b), which are multi-label multi-class classification tasks. The goal is to classify short texts describing German books into one or multiple classes, 8 generic categories for Subtask (a) and 343 specific categories for Subtask (b). Our system comprises of three stages. (a) Transform multi-label multi-class problem into single-label multi-class problem. Build a category model. (b) Build a class count model to predict the number of classes a given input belongs to. (c) Transform single-label problem into multi-label problem back again by selecting the top-k predictions from the category model, with the optimal k value predicted from the class count model. Our approach utilizes a Support Vector Classification model on the extracted vectorized tf-idf features by leveraging the Byte-Pair Token Encoding (BPE), and reaches f1-micro scores of 0.857 in the test evaluation phase and 0.878 in post evaluation phase for Subtask (a), while 0.395 in post evaluation phase for Subtask (b) of the competition. We have provided our solution code in the following link: https://github.com/oneraghavan/germeval-2019.</i></p>

<pre xml:space="preserve">
@inproceedings{raghavan_konvens19,
  author = {Raghavan, AK and Umaashankar, Venkatesh
  and Gudur, Gautham Krishna},
  title = {Label Frequency Transformation for 
  Multi-Label Multi-Class Text Classification},
  booktitle = {Proceedings of the 15th Conference on 
  Natural Language Processing (KONVENS 2019)},
  pages = {341--346},
  year = {2019},
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dl.acm.org/doi/10.1145/3325413.3329790" target="_blank"><img src="images/EMDL19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://dl.acm.org/doi/10.1145/3325413.3329790" id="EMDL19" target="_blank">
      <heading>ActiveHARNet: Towards On-Device Deep Bayesian Active Learning for Human Activity Recognition</heading>
      </a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Prahalathan Sundaramoorthy, Venkatesh Umaashankar<br>
      3rd International Workshop on Embedded and Mobile Deep Learning (EMDL), <b>ACM MobiSys 2019</b> <i>[Oral Presentation]</i><br>
      Also presented as a <a href="https://www.youtube.com/watch?v=Kfy0URcPxyE&t" target="_blank">poster</a> at <b>EEML 2020</b>
      </p>

      <div class="paper" id="emdl19">
      <a href="https://arxiv.org/pdf/1906.00108.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('emdl19_abs')">abstract</a> /
      <a href="https://github.com/gauthamkrishna-g/ActiveHARNet" target="_blank">code</a> /
      <a href="https://www.youtube.com/watch?v=Kfy0URcPxyE&t" target="_blank"">video</a> /
      <a href="slides/EMDL19_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('emdl19')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="emdl19_abs">Various health-care applications such as assisted living, fall detection etc., require modeling of user behavior through Human Activity Recognition (HAR). HAR using mobile- and wearable-based deep learning algorithms have been on the rise owing to the advancements in pervasive computing. However, there are two other challenges that need to be addressed: first, the deep learning model should support on-device incremental training (model updation) from real-time incoming data points to learn user behavior over time, while also being resource-friendly; second, a suitable ground truthing technique (like Active Learning) should help establish labels on-the-fly while also selecting only the most informative data points to query from an oracle. Hence, in this paper, we propose ActiveHARNet, a resource-efficient deep ensembled model which supports on-device Incremental Learning and inference, with capabilities to represent model uncertainties through approximations in Bayesian Neural Networks using dropout. This is combined with suitable acquisition functions for active learning. Empirical results on two publicly available wrist-worn HAR and fall detection datasets indicate that ActiveHARNet achieves considerable efficiency boost during inference across different users, with a substantially low number of acquired pool points (at least 60% reduction) during incremental learning on both datasets experimented with various acquisition functions, thus demonstrating deployment and Incremental Learning feasibility.</i></p>

<pre xml:space="preserve">
@inproceedings{gudur_emdl19,
  author = {Gudur, Gautham Krishna and 
  Sundaramoorthy, Prahalathan and 
  Umaashankar, Venkatesh},
  title = {ActiveHARNet: Towards On-Device 
  Deep Bayesian Active Learning for Human 
  Activity Recognition},
  booktitle = {The 3rd International Workshop 
  on Deep Learning for Mobile Systems and 
  Applications},
  pages = {7--12},
  numpages = {6},
  year = {2019}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://dl.acm.org/doi/10.1145/3212725.3212728" target="_blank"><img src="images/EMDL18.png" alt="sym" width="75%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://dl.acm.org/doi/10.1145/3212725.3212728" id="EMDL18" target="_blank">
      <heading>HARNet: Towards On-Device Incremental Learning using Deep Ensembles on Constrained Devices</heading>
      </a><br>
      Prahalathan Sundaramoorthy, <b><u>Gautham Krishna Gudur</u></b>, Manav Rajiv Moorthy, R Nidhi Bhandari, Vineeth Vijayaraghavan<br>
      2nd International Workshop on Embedded and Mobile Deep Learning (EMDL), <b>ACM MobiSys 2018</b> <i>[Oral Presentation]</i>
      </p>

      <div class="paper" id="emdl18">
      <a href="https://www.sigmobile.org/mobisys/2018/workshops/deepmobile18/papers/HARNet.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('emdl18_abs')">abstract</a> /
      <a href="https://github.com/gauthamkrishna-g/HARNet" target="_blank">code</a> /
      <a href="slides/EMDL18_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('emdl18')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="emdl18_abs">Recent advancements in the domain of pervasive computing have seen the incorporation of sensor-based Deep Learning algorithms in Human Activity Recognition (HAR). Contemporary Deep Learning models are engineered to alleviate the difficulties posed by conventional Machine Learning algorithms which require extensive domain knowledge to obtain heuristic hand-crafted features. Upon training and deployment of these Deep Learning models on ubiquitous mobile/embedded devices, it must be ensured that the model adheres to their computation and memory limitations, in addition to addressing the various mobile- and user-based heterogeneities prevalent in actuality. To handle this, we propose HARNet - a resource-efficient and computationally viable network to enable on-line Incremental Learning and User Adaptability as a mitigation technique for anomalous user behavior in HAR. Heterogeneity Activity Recognition Dataset was used to evaluate HARNet and other proposed variants by utilizing acceleration data acquired from diverse mobile platforms across three different modes from a practical application perspective. We perform Decimation as a Down-sampling technique for generalizing sampling frequencies across mobile devices, and Discrete Wavelet Transform for preserving information across frequency and time. Systematic evaluation of HARNet on User Adaptability yields an increase in accuracy by ~35% by leveraging the model's capability to extract discriminative features across activities in heterogeneous environments.</i></p>

<pre xml:space="preserve">
@inproceedings{sundaramoorthy_emdl18,
  author = {Sundaramoorthy, Prahalathan and 
  Gudur, Gautham Krishna and Moorthy, 
  Manav Rajiv and Bhandari, R Nidhi and 
  Vijayaraghavan, Vineeth},
  title = {HARNet: Towards On-Device 
  Incremental Learning Using Deep Ensembles 
  on Constrained Devices},
  booktitle = {Proceedings of the 2nd International 
  Workshop on Embedded and Mobile Deep Learning},
  pages = {31--36},
  numpages = {6}
  year = {2018}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://link.springer.com/chapter/10.1007/978-3-030-03405-4_42" target="_blank"><img src="images/FICC18.png" alt="sym" width="75%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://link.springer.com/chapter/10.1007/978-3-030-03405-4_42" id="FICC18" target="_blank">
      <heading>A Generic Multi-modal Dynamic Gesture Recognition System using Machine Learning</heading>
      </a><br>
      <b><u>Gautham Krishna G</u></b>, Karthik Subramanian Nathan, Yogesh Kumar B, Ankith A Prabhu, Ajay Kannan, Vineeth Vijayaraghavan<br>
      <b>IEEE FICC 2018</b>
      </p>

      <div class="paper" id="ficc18">
      <a href="https://arxiv.org/pdf/1809.05839.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('ficc18_abs')">abstract</a> /
      <a href="https://github.com/gauthamkrishna-g/Dynamic-Gesture-Recognition" target="_blank">code</a> /
      <a href="slides/FICC18_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('ficc18')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="ficc18_abs">Human computer interaction facilitates intelligent communication between humans and computers, in which gesture recognition plays a prominent role. This paper proposes a machine learning system to identify dynamic gestures using triaxial acceleration data acquired from two public datasets. These datasets, uWave and Sony, were acquired using accelerometers embedded in Wii remotes and smartwatches, respectively. A dynamic gesture signed by the user is characterized by a generic set of features extracted across time and frequency domains. The system was analyzed from an end-user perspective and was modelled to operate in three modes. The modes of operation determine the subsets of data to be used for training and testing the system. From an initial set of seven classifiers, three were chosen to evaluate each dataset across all modes rendering the system towards mode-neutrality and dataset-independence. The proposed system is able to classify gestures performed at varying speeds with minimum preprocessing, making it computationally efficient. Moreover, this system was found to run on a low-cost embedded platform – Raspberry Pi Zero (USD 5), making it economically viable.</i></p>

<pre xml:space="preserve">
@inproceedings{krishna_ficc18,
  author = {Krishna, G Gautham and Nathan,
  Karthik Subramanian and Kumar, B Yogesh 
  and Prabhu, Ankith A and Kannan, Ajay and
  Vijayaraghavan, Vineeth},
  title = {A Generic Multi-modal Dynamic Gesture
  Recognition System Using Machine Learning},
  booktitle = {Future of Information and 
  Communication Conference},
  pages = {603--615},
  year = {2018},
  organization = {Springer}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://link.springer.com/chapter/10.1007/978-981-10-5780-9_13" target="_blank"><img src="images/ICAICR17.jpeg" alt="sym" width="50%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="https://link.springer.com/chapter/10.1007/978-981-10-5780-9_13" id="EEG_ICAICR17" target="_blank">
      <heading>Electroencephalography Based Analysis of Emotions Among Indian Film Viewers</heading>
      </a><br>
      <b><u>Gautham Krishna G</u></b>, G Krishna, N Bhalaji<br>
      <b>ICAICR 2017</b>, Springer
      </p>

      <div class="paper" id="eeg_icaicr17">
      <a href="publications/Neurocinematics_ICAICR17.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('eeg_icaicr17_abs')">abstract</a> /
      <a href="slides/ICAICR17_Slides.pdf" target="_blank">slides</a> /
      <a shape="rect" href="javascript:togglebib('eeg_icaicr17')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="eeg_icaicr17_abs">The film industry has been a major factor in the rapid growth of the Indian entertainment industry. While watching a film, the viewers undergo an experience that evolves over time, thereby grabbing their attention. This triggers a sequence of processes which is perceptual, cognitive and emotional. Neurocinematics is an emerging field of research, that measures the cognitive responses of a film viewer. Neurocinematic studies, till date, have been performed using functional magnetic resonance imaging (fMRI); however recent studies have suggested the use of advancements in electroencephalography (EEG) in neurocinematics to address the issues involved with fMRI. In this article the emotions corresponding to two different genres of Indian films are captured with the real-time brainwaves of viewers using EEG and analyzed using R language.</i></p>

<pre xml:space="preserve">
@inproceedings{krishna_icaicr17,
  author={G, Gautham Krishna and Krishna, G and 
  Bhalaji, N},
  title = {Electroencephalography Based Analysis 
  of Emotions Among Indian Film Viewers},
  booktitle = {International Conference on Advanced
  Informatics for Computing Research},
  pages = {145--155},
  year = {2017},
  organization = {Springer}
}
</pre>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://www.sciencedirect.com/science/article/pii/S1877050916305002" target="_blank"><img src="images/ICRTCSE16.png" alt="sym" width="80%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <a href="http://www.sciencedirect.com/science/article/pii/S1877050916305002" id="RPL_ICRTCSE16" target="_blank">
      <heading>Analysis of Routing Protocol for Low-power & Lossy Networks in IoT Real Time Applications</heading>
      </a><br>
      <b><u>Gautham Krishna G</u></b>, G Krishna, N Bhalaji<br>
      <b>ICRTCSE 2016</b>, Procedia Computer Science
      </p>

      <div class="paper" id="rpl_icrtcse16">
      <a href="publications/RPLIoT_ICRTCSE16.pdf" target="_blank">pdf</a> /
      <a href="javascript:toggleblock('rpl_icrtcse16_abs')">abstract</a> /
      <a shape="rect" href="javascript:togglebib('rpl_icrtcse16')" class="togglebib">bibtex</a>

      <p align="justify"> <i id="rpl_icrtcse16_abs">The wide-scaled sensing by Wireless Sensor Networks (WSN) has impacted several areas in the modern generation. It has offered the ability to measure, observe and understand the various physical factors from our environment. The rapid increase of WSN devices in an actuating-communicating network has led to the evolution of Internet of Things (IoT), where information is shared seamlessly across platforms by blending the sensors and actuators with our environment. These low cost WSN devices provide automation in medical and environmental monitoring. Evaluating the performance of these sensors using RPL enhances their use in real world applications. The realization of these RPL performances from different nodes focuses our study to utilize WSNs in our day-to-day applications. The effective sensor nodes (motes) for the appropriate environmental scenarios are analyzed, and we propose a collective view of the metrics for the same, for enhanced throughput in the given field of usage.</i></p>

<pre xml:space="preserve">
@article{krishna_icrtcse16,
  author = {Krishna, G Gautham and Krishna, G 
  and Bhalaji, N},
  title = {Analysis of Routing Protocol for Low-power
  and Lossy Networks in IoT Real Time Applications},
  journal = {Procedia Computer Science},
  volume = {87},
  pages = {270--274},
  year = {2016},
  publisher={Elsevier}
}
</pre>

      </div>
    </td>
  </tr>

</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><heading id="poster">&nbsp;&nbsp;Poster/Extended Abstract</heading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://mobiuk.org/" target="_blank"><img src="images/INTERSPEECH21.png" alt="sym" width="65%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://mobiuk.org/2021/programme.html" id="MOBIUK21" target="_blank">
      <heading>Heterogeneous Zero-Shot Federated Learning with New Classes for On-Device Audio Classification</heading></a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Satheesh Kumar Perepu<br>
      <b>MobiUK 2021</b> (Third UK Mobile, Wearable and Ubiquitous Systems Research Symposium)
      </p>

      <div class="paper" id="mobiuk21">
      <a href="https://mobiuk.org/2021/abstract/S1-P2_Gudur_HeterogeneousZeroShotFederatedLearning.pdf" target="_blank">extended abstract</a> /
      <a href="https://youtu.be/Nz-SDw0kfhc?t=1195" target="_blank">video</a> /
      <a href="slides/MobiUK21_Slides.pdf" target="_blank">slides</a>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="http://bayesiandeeplearning.org/" target="_blank"><img src="images/BDL20.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="http://bayesiandeeplearning.org/" id="ALHEALTH_BDL20" target="_blank">
      <heading>Bayesian Active Learning for Wearable and Mobile Health</heading></a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Abhijith Ragav, Prahalathan Sundaramoorthy, Venkatesh Umaashankar<br>
      <b>BDL 2020</b> (<b>NeurIPS</b> Europe meetup on Bayesian Deep Learning)
      </p>

      <div class="paper" id="alhealth_bdl20">
      <a href="publications/ALMobileHealth_BDL_NeurIPS20.pdf" target="_blank">poster</a>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><a href="https://mobiuk.org/programme2019.html" target="_blank"><img src="images/EMDL19.png" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p><a href="https://mobiuk.org/programme2019.html" id="MOBIUK19" target="_blank">
      <heading>Handling Real-time Unlabeled Data in Activity Recognition using Deep Bayesian Active Learning and Data Programming</heading></a><br>
      <b><u>Gautham Krishna Gudur</u></b>, Prahalathan Sundaramoorthy, Venkatesh Umaashankar<br>
      <b>MobiUK 2019</b> (Second UK Mobile, Wearable and Ubiquitous Systems Research Symposium), University of Oxford
      </p>

      <div class="paper" id="mobiuk19">
      <a href="https://mobiuk.org/2019/abstract/S5-P4_Gudur_HandlingRealTimeUnlabeledData.pdf" target="_blank">extended abstract</a> /
      <a href="slides/MobiUK19_Slides.pdf" target="_blank">slides</a>

      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><img src="images/CBC15.png" alt="sym" width="60%" style="border-radius:15px"></td>
    <td width="67%" valign="top">
      <p><a href="publications/Neurocinematics_CBC15_Poster.pdf" id="NEUROCINEMATICS_CBC15" target="_blank">
      <heading>Neurocinematics: The Intelligent Review System</heading></a><br>
      N Bhalaji, G Krishna, <b><u>G Gautham Krishna</u></b><br>
      <b>CBC 2015</b> (3rd International Conference on Cognition, Brain and Computation), IIT Gandhinagar
      </p>

      <div class="paper" id="neurocinematics_cbc15">
      <a href="publications/Neurocinematics_CBC15_Poster.pdf" target="_blank">poster</a> /
      <a href="slides/CBC15_Slides.pdf" target="_blank">slides</a>

      </div>
    </td>
  </tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading id="patents">&nbsp;&nbsp;Patents</sectionheading></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td width="33%" valign="top"><img src="images/patents.jpeg	" alt="sym" width="100%" style="border-radius:15px"></a></td>
    <td width="67%" valign="top">
      <p>
      <ul>
        <li> <a href="https://patents.google.com/patent/WO2022013879A1/" id="FL_Heterogeneous_Patent" target="_blank">Federated Learning using Heterogeneous Labels</a></li>
        <li> <a href="https://patents.google.com/patent/WO2022162677A1/" id="FL_Newlabel_Patent" target="_blank">Distributed Machine Learning with New Labels using Heterogeneous Label Distribution</a></li>
        <li> <a href="https://patents.google.com/patent/WO2023166515A1/" id="AD_Patent" target="_blank">Method and Apparatus for Approach Recommendation with Threshold Optimization in Unsupervised Anomaly Detection</a></li>
      </ul>
      </p>
    </td>
  </tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading id="services">&nbsp;&nbsp;Services</sectionheading>
    <ul>
      <li> Program Committee Member/Reviewer</li>
      <ul>
        <li> ICML 2024 - Efficient Systems for Foundation Models Workshop (ES-FoMo)</li>
        <li> ICLR 2021 - Distributed and Private Machine Learning Workshop (DPML)</li> 
        <li> NeurIPS - Machine Learning for Health Workshop (ML4H)<br>
            - ML4H 2020, ML4H 2019
        </li>
        <li> KONVENS 2019, GermEval</li> 
      </ul>
      <li> Technical Reviewer of the book titled "Hands-On Meta Learning With Python"</li>
      <li> Event Organizer of "Data Nuggets" - a Data Science event, Invente 2016</li>
    </ul>
  </td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id="honorsandawards">&nbsp;&nbsp;Honors and Awards</sectionheading>
    <ul>
		<li> Our project AIB (Automated Intelligent knowledge Base) won <b>Ericsson's Top Performance Competition 2020</b> in the Operational Excellence category</li>
      <li> <a href="https://www.hackerrank.com/certificates/014169e8e7b2" target="_blank"><b>Top 1 percentile</b></a> in <a href="https://www.hackerrank.com/gauthamkrishna_g" target="_blank">HackerRank</a> (Algorithms Domain/Problem Solving - Advanced)</b></li>
      <li> Full financial registration grant to attend ICLR 2021, NeurIPS 2020 and OxML 2020</li>
      <li> <b>Winner</b> of GermEval Shared Task 1 Challenge - Subtask (a) @ KONVENS 2019 in post-evaluation phase</li>
      <li> Undergraduate financial research grant of <b>INR 25,000</b> from SSN College of Engineering</li>
      <li> Certification of Merit for Grade A1 in all subjects in AISSE (CBSE 10th boards)</li>
      <li> Completed all 10 levels of UCMAS Mental Arithmetic (Abacus)</li>
      <li> Division Level Badminton Player (U-19)</li>
      <li> 29th Rank overall in Grade 3 Keyboard</li>
    </ul>
  </td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id="talks">&nbsp;&nbsp;Talks</sectionheading>
    <ul>
      <li> Machine Learning and Ubiquitous Computing (June 2022)<br>
      SSN College of Engineering
      <li> Heterogeneous Zero-Shot Federated Learning with New Classes for On-Device Audio Classification (July 2021)<br>
      MobiUK 2021
      <li> Telecom-Specific Language Translation using GCP (May 2021)<br>
      Ericsson/Google Cloud Day
      <li> Resource-Constrained Machine Learning for Ubiquitous Computing Applications (Sept 2020)<br>
      <a href="images/flippedgaius20.jpeg" target="_blank">Flipped by GAIUS</a> /
      <a href="slides/FlippedGauis20_Slides.pdf" target="_blank">slides</a></li>      
    </ul>
  </td></tr>
</table>


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id="summer_schools">&nbsp;&nbsp;Summer Schools</sectionheading>
    <ul>
      <li> <b><a href="summer_schools/SSAI2021_Certificate.pdf" target="_blank">5th Summer School on Artificial Intelligence</a></b> (Aug 2022)<br>
      Organizers – International Institute of Information Technology - Hyderabad (Remote – CVIT, IIIT-H)<br>
      Focus Areas – Computer Vision and Machine Learning
      <li> <b><a href="summer_schools/EEML2021_Certificate.pdf" target="_blank">Eastern European Machine Learning Summer School (EEML 2021)</a></b> (July 2021)<br>
      Organizers – DeepMind and others (Virtual – Budapest, Hungary)<br>
      Presented my work on <a href="https://www.youtube.com/watch?v=U_aWp6wajzM" target="_blank">Zero-shot Federated Learning with New Classes</a> as a poster.<br> Presented our idea on task-independent continual learning at the unconference sessions.
      <li> <b><a href="summer_schools/OxML2020_Certificate.pdf" target="_blank">Oxford Machine Learning Summer School (OxML 2020)</a></b> (Aug 2020)<br>
      Organizers – University of Oxford, AI for Global Goals, CIFAR, Saïd Business School, Deep Medicine<br>
      Attended with <b>full-fee waiver</b>.<br>
      Focus Areas – Deep Learning and Healthcare
      <li> <b><a href="summer_schools/EEML2020_Certificate.pdf" target="_blank">Eastern European Machine Learning Summer School (EEML 2020)</a></b> (July 2020)<br>
      Organizers – DeepMind and others (Virtual – Krakow, Poland)<br>
      Presented my work on <a href="https://www.youtube.com/watch?v=Kfy0URcPxyE&t" target="_blank">ActiveHARNet</a> as a poster.
    </ul>
  </td></tr>
</table>

    
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td>
    <sectionheading id="certifications">&nbsp;&nbsp;MOOCs/Certifications</sectionheading>
    <ul>
      <li> <a href="https://www.hackerrank.com/gauthamkrishna_g" target="_blank">HackerRank</a></li>
      <ul>
        <li> Problem Solving – <a href="https://www.hackerrank.com/certificates/014169e8e7b2" target="_blank">Advanced</a>, <a href="https://www.hackerrank.com/certificates/f99df227514e" target="_blank">Intermediate</a>, <a href="https://www.hackerrank.com/certificates/911b79a57138" target="_blank">Basic</a></li>
      </ul>
      <li> University of Washington, Coursera</li>
      &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.coursera.org/account/accomplishments/specialization/certificate/DSAS23JZKGG6" target="_blank">Machine Learning Specialization (4 courses)</a>
      <ul>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/KBN26TGYRFS2" target="_blank">Machine Learning Foundations: A Case Study Approach</a></li>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/6MD63T837DRW" target="_blank">Machine Learning: Regression</a></li>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/6WG9X8M3XDCD" target="_blank">Machine Learning: Classification</a></li>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/MM93FX3USFY3" target="_blank">Machine Learning: Clustering & Retrieval</a></li>
      </ul>
      <li> National Research University Higher School of Economics, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/" target="_blank">Bayesian Methods for Machine Learning</a></li>
      </ul>
      <li> University of Alberta, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/" target="_blank">Fundamentals of Reinforcement Learning</a></li>
      </ul>
      <li> Stanford University, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/JCU22MWZSVHS" target="_blank">Machine Learning</a></li>
      </ul>
      <li> University of California San Diego, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/DEUBHDVKGD3A" target="_blank">Algorithmic Toolbox</a></li>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/NULMGEKZ5CSY" target="_blank">Data Structures</a></li>
      </ul>
      <li> John Hopkins University, Coursera</li>
      <ul>
        <li> <a href="https://www.coursera.org/account/accomplishments/certificate/Y69XRGC2M35V" target="_blank">R Programming</a></li>
      </ul>
      <li> Google, Udacity</li>
      <ul>
        <li> <a href="https://in.udacity.com/course/deep-learning--ud730" target="_blank">Deep Learning (ud370)</a></li>
      </ul>
      <li> Stanford University</li>
      <ul>
        <li> <a href="http://cs231n.stanford.edu/" target="_blank">CS231n: Convolutional Neural Networks for Visual Recognition</a></li>
      </ul>
      <li> Udemy</li>
      <ul>
        <li> <a href="https://www.udemy.com/certificate/UC-ZJTYGNIY/" target="_blank">Udemy The Complete Ethical Hacking Course: Beginner to Advanced!</a></li>
      </ul>
    </ul>
  </td></tr>
</table>

<!--Countries visited so far: India, UK, South Korea, Germany, Switzerland, France, Italy, Vatican, Luxembourg, Singapore, Malaysia, Thailand, SriLanka.-->


<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right"><font size="1.5">
    Template modified from <a href="http://www.cs.berkeley.edu/~barron/" target="_blank">this</a>, <a href="https://www.cs.cmu.edu/~dpathak/" target="_blank">this</a>, <a href="https://homes.cs.washington.edu/~kusupati/">this</a> and <a href="http://jeffdonahue.com/">this</a>.
    </font></p></td></tr>
</table>


</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('hill22_calib_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('svft24_abs');
  </script>
<script xml:space="preserve" language="JavaScript">
hideblock('icmla22_ad_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('interspeech21_newclassfl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('mlmh20_stress_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('dlhar20_heterofdl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('aiot20_heterofdl_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('icdmw19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('purba19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('germeval19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('emdl19_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('emdl18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ficc18_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('eeg_icaicr17_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('rpl_icrtcse16_abs');
</script>

</body>

</html>

